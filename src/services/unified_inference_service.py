"""ç»Ÿä¸€æ¨ç†æœåŠ¡ - æ”¯æŒé›†æˆæ¨¡å¼å’Œè¿œç¨‹ä»£ç†æ¨¡å¼

æ ¹æ®é…ç½®å†³å®šæ˜¯ç›´æ¥è°ƒç”¨é›†æˆæ¨¡å‹ï¼Œè¿˜æ˜¯é€šè¿‡WebSocketä»£ç†åˆ°è¿œç¨‹è¿›ç¨‹
"""

import importlib
import logging
from typing import Any, Dict, List, Optional
from concurrent.futures import Future, ThreadPoolExecutor

from ..constants import EventTopic
from ..core.event_bus import Event, EventBus
from ..models.base_inference_model import BaseInferenceModel

try:
    from ..interfaces.model_ws_client import ModelBackendClient
    HAS_CLIENT = True
except ImportError:
    HAS_CLIENT = False


class UnifiedInferenceService:
    """ç»Ÿä¸€æ¨ç†æœåŠ¡
    
    æ”¯æŒä¸¤ç§æ¨ç†æ¨¡å¼ï¼š
    1. integrated: æ¨¡å‹ç›´æ¥é›†æˆåœ¨è¿›ç¨‹ä¸­
    2. remote: é€šè¿‡WebSocketä»£ç†åˆ°ç‹¬ç«‹è¿›ç¨‹
    
    å¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­çµæ´»åˆ‡æ¢æ¨¡å¼ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
    """
    
    def __init__(
        self,
        bus: EventBus,
        model_configs: List[Dict[str, Any]],
        *,
        logger: Optional[logging.Logger] = None
    ):
        """åˆå§‹åŒ–ç»Ÿä¸€æ¨ç†æœåŠ¡
        
        Args:
            bus: äº‹ä»¶æ€»çº¿
            model_configs: æ¨¡å‹é…ç½®åˆ—è¡¨
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.bus = bus
        self.model_configs = model_configs
        self.logger = logger or logging.getLogger("service.inference")
        
        # é›†æˆæ¨¡å¼çš„æ¨¡å‹å®ä¾‹
        self.integrated_models: Dict[str, BaseInferenceModel] = {}
        
        # è¿œç¨‹æ¨¡å¼çš„å®¢æˆ·ç«¯å®ä¾‹
        self.remote_clients: Dict[str, "ModelBackendClient"] = {}
        
        # çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥å¤„ç†
        self._executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="inference")
        
        self._running = False
    
    def start(self) -> None:
        """å¯åŠ¨æ¨ç†æœåŠ¡"""
        if self._running:
            self.logger.warning("æ¨ç†æœåŠ¡å·²åœ¨è¿è¡Œ")
            return
        
        self.logger.info("å¯åŠ¨ç»Ÿä¸€æ¨ç†æœåŠ¡...")
        
        enabled_count = 0
        for config in self.model_configs:
            if not config.get("enabled", True):
                self.logger.info(f"è·³è¿‡ç¦ç”¨çš„æ¨¡å‹: {config.get('name')}")
                continue
            
            model_name = config["name"]
            model_type = config["type"]
            mode = config.get("mode", "remote")
            
            if mode == "integrated":
                # é›†æˆæ¨¡å¼ï¼šç›´æ¥åŠ è½½æ¨¡å‹
                enabled_count += self._start_integrated_model(model_name, model_type, config)
            elif mode == "remote":
                # è¿œç¨‹æ¨¡å¼ï¼šè¿æ¥åˆ°æ¨¡å‹åç«¯
                enabled_count += self._start_remote_client(model_name, model_type, config)
            else:
                self.logger.error(f"æœªçŸ¥çš„æ¨¡å‹æ¨¡å¼: {mode} (æ¨¡å‹: {model_name})")
        
        if enabled_count == 0:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            return
        
        # è®¢é˜…éœ€è¦æ¨ç†çš„äº‹ä»¶
        self.bus.subscribe(EventTopic.MULTIMODAL_SNAPSHOT, self._on_multimodal_data)
        self.bus.subscribe(EventTopic.EMOTION_REQUEST, self._on_emotion_request)
        self.bus.subscribe(EventTopic.EEG_REQUEST, self._on_eeg_request)
        self.logger.info(f"å·²è®¢é˜…äº‹ä»¶: {EventTopic.MULTIMODAL_SNAPSHOT.value}, {EventTopic.EMOTION_REQUEST.value}, {EventTopic.EEG_REQUEST.value}")
        
        self._running = True
        self.logger.info(f"âœ… ç»Ÿä¸€æ¨ç†æœåŠ¡å·²å¯åŠ¨ (å…± {enabled_count} ä¸ªæ¨¡å‹)")
    
    def _start_integrated_model(
        self,
        model_name: str,
        model_type: str,
        config: Dict[str, Any]
    ) -> int:
        """å¯åŠ¨é›†æˆæ¨¡å¼çš„æ¨¡å‹
        
        Returns:
            1 if success, 0 if failed
        """
        integrated_config = config.get("integrated", {})
        class_path = integrated_config.get("class")
        options = integrated_config.get("options", {})
        
        if not class_path:
            self.logger.error(f"é›†æˆæ¨¡å‹ç¼ºå°‘ class é…ç½®: {model_name}")
            return 0
        
        try:
            # åŠ¨æ€å¯¼å…¥æ¨¡å‹ç±»
            module_name, _, class_name = class_path.rpartition(".")
            if not module_name.startswith("src."):
                module_name = f"src.{module_name}"
            
            module = importlib.import_module(module_name)
            model_class = getattr(module, class_name)
            
            # å®ä¾‹åŒ–å¹¶åŠ è½½æ¨¡å‹
            model = model_class(model_name, logger=self.logger, **options)
            model.load()
            
            self.integrated_models[model_type] = model
            self.logger.info(f"âœ… é›†æˆæ¨¡å‹å·²åŠ è½½: {model_name} ({model_type})")
            return 1
            
        except Exception as e:
            self.logger.error(f"åŠ è½½é›†æˆæ¨¡å‹å¤±è´¥ ({model_name}): {e}", exc_info=True)
            return 0
    
    def _start_remote_client(
        self,
        model_name: str,
        model_type: str,
        config: Dict[str, Any]
    ) -> int:
        """å¯åŠ¨è¿œç¨‹æ¨¡å¼çš„å®¢æˆ·ç«¯
        
        Returns:
            1 if success, 0 if failed
        """
        if not HAS_CLIENT:
            self.logger.error(f"WebSocketå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨è¿œç¨‹æ¨¡å‹: {model_name}")
            return 0
        
        remote_config = config.get("remote", {})
        host = remote_config.get("host", "127.0.0.1")
        port = remote_config.get("port", 8766)
        url = f"ws://{host}:{port}"
        
        reconnect = remote_config.get("reconnect", True)
        reconnect_interval = remote_config.get("reconnect_interval", 5.0)
        
        try:
            client = ModelBackendClient(
                model_type,
                url,
                reconnect=reconnect,
                reconnect_interval=reconnect_interval
            )
            client.start()
            
            self.remote_clients[model_type] = client
            self.logger.info(f"âœ… è¿œç¨‹æ¨¡å‹å®¢æˆ·ç«¯å·²å¯åŠ¨: {model_name} ({model_type}) -> {url}")
            return 1
            
        except Exception as e:
            self.logger.error(f"å¯åŠ¨è¿œç¨‹æ¨¡å‹å®¢æˆ·ç«¯å¤±è´¥ ({model_name}): {e}", exc_info=True)
            return 0
    
    def stop(self) -> None:
        """åœæ­¢æ¨ç†æœåŠ¡"""
        if not self._running:
            return
        
        self.logger.info("åœæ­¢ç»Ÿä¸€æ¨ç†æœåŠ¡...")
        
        # å–æ¶ˆè®¢é˜…
        try:
            self.bus.unsubscribe(EventTopic.MULTIMODAL_SNAPSHOT, self._on_multimodal_data)
            self.bus.unsubscribe(EventTopic.EMOTION_REQUEST, self._on_emotion_request)
            self.bus.unsubscribe(EventTopic.EEG_REQUEST, self._on_eeg_request)
        except Exception as e:
            self.logger.error(f"å–æ¶ˆè®¢é˜…å¤±è´¥: {e}")
        
        # å¸è½½é›†æˆæ¨¡å‹
        for model_type, model in self.integrated_models.items():
            try:
                model.unload()
                self.logger.info(f"å·²å¸è½½é›†æˆæ¨¡å‹: {model_type}")
            except Exception as e:
                self.logger.error(f"å¸è½½é›†æˆæ¨¡å‹å¤±è´¥ ({model_type}): {e}")
        self.integrated_models.clear()
        
        # åœæ­¢è¿œç¨‹å®¢æˆ·ç«¯
        for model_type, client in self.remote_clients.items():
            try:
                client.stop()
                self.logger.info(f"å·²åœæ­¢è¿œç¨‹å®¢æˆ·ç«¯: {model_type}")
            except Exception as e:
                self.logger.error(f"åœæ­¢è¿œç¨‹å®¢æˆ·ç«¯å¤±è´¥ ({model_type}): {e}")
        self.remote_clients.clear()
        
        # å…³é—­çº¿ç¨‹æ± 
        self._executor.shutdown(wait=True)
        
        self._running = False
        self.logger.info("âœ… ç»Ÿä¸€æ¨ç†æœåŠ¡å·²åœæ­¢")
    
    def _on_multimodal_data(self, event: Event) -> None:
        """å¤„ç†å¤šæ¨¡æ€æ•°æ®,åˆ†å‘åˆ°å„æ¨¡å‹"""
        payload = event.payload or {}
        
        # æå–æ•°æ®
        status = payload.get("status", "idle")
        timestamp = payload.get("timestamp")
        frame_count = payload.get("frame_count", 0)
        elapsed_time = payload.get("elapsed_time", 0.0)
        
        # ä¼˜å…ˆä½¿ç”¨å†…å­˜æ¨¡å¼(é¿å…é‡å¤I/O)
        memory_mode = payload.get("memory_mode", False)
        file_mode = payload.get("file_mode", False)
        
        # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
        if memory_mode:
            # å†…å­˜æ¨¡å¼: ç›´æ¥ä½¿ç”¨numpyæ•°ç»„
            rgb_frames_memory = payload.get("rgb_frames_memory", [])
            depth_frames_memory = payload.get("depth_frames_memory", [])
            eyetrack_memory = payload.get("eyetrack_memory", [])
            
            if not rgb_frames_memory:
                # æ²¡æœ‰RGBå¸§æ•°æ®æ—¶,é™é»˜è·³è¿‡
                return
            
            # å†…å­˜æ¨¡å¼ä¸éœ€è¦æ–‡ä»¶è·¯å¾„
            rgb_video_path = None
            depth_video_path = None
            eyetrack_json_path = None
            rgb_frames_b64 = []
            depth_frames_b64 = []
            eyetrack_samples = []
            
        elif file_mode:
            # æ–‡ä»¶æ¨¡å¼:æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨
            rgb_video_path = payload.get("rgb_video_path")
            depth_video_path = payload.get("depth_video_path")
            eyetrack_json_path = payload.get("eyetrack_json_path")
            
            if not rgb_video_path:
                # æ²¡æœ‰RGBè§†é¢‘æ–‡ä»¶æ—¶,é™é»˜è·³è¿‡
                return
                
            # ä½¿ç”¨æ–‡ä»¶è·¯å¾„è¿›è¡Œæ¨ç†
            rgb_frames_memory = []
            depth_frames_memory = []
            eyetrack_memory = []
            rgb_frames_b64 = []
            depth_frames_b64 = []
            eyetrack_samples = []
        else:
            # Base64æ¨¡å¼:æå–å¤šå¸§åºåˆ—æ•°æ®
            rgb_frames_b64 = payload.get("rgb_frames_b64", [])
            depth_frames_b64 = payload.get("depth_frames_b64", [])
            eyetrack_samples = payload.get("eyetrack_samples", [])
            rgb_video_path = None
            depth_video_path = None
            eyetrack_json_path = None
            rgb_frames_memory = []
            depth_frames_memory = []
            eyetrack_memory = []
            
            if not rgb_frames_b64:
                # æ²¡æœ‰RGBå¸§åºåˆ—æ•°æ®æ—¶,é™é»˜è·³è¿‡
                return
        
        # æ£€æŸ¥é‡‡é›†çŠ¶æ€
        if status != "running":
            return
        
        # éªŒè¯å¸§æ•°é‡æ˜¯å¦è¶³å¤Ÿ(é¿å…æ¨¡å‹æ¨ç†å¤±è´¥)
        # ç–²åŠ³åº¦æ¨¡å‹éœ€è¦è¶³å¤Ÿçš„å¸§åºåˆ—(è‡³å°‘30å¸§)
        MIN_FRAMES_FOR_FATIGUE = 30
        
        # æ£€æŸ¥å¸§æ•°æ˜¯å¦è¶³å¤Ÿ
        if memory_mode:
            # å†…å­˜æ¨¡å¼: æ£€æŸ¥æ•°ç»„é•¿åº¦
            if len(rgb_frames_memory) < MIN_FRAMES_FOR_FATIGUE:
                # æ•°æ®ä¸è¶³æ—¶é™é»˜è·³è¿‡
                return
        elif file_mode:
            # æ–‡ä»¶æ¨¡å¼: æ£€æŸ¥ frame_count å…ƒæ•°æ®
            if frame_count < MIN_FRAMES_FOR_FATIGUE:
                # æ•°æ®ä¸è¶³æ—¶é™é»˜è·³è¿‡,é¿å…æ—¥å¿—åˆ·å±
                return
        else:
            # Base64æ¨¡å¼: æ£€æŸ¥æ•°ç»„é•¿åº¦
            if len(rgb_frames_b64) < MIN_FRAMES_FOR_FATIGUE:
                # æ•°æ®ä¸è¶³æ—¶é™é»˜è·³è¿‡
                return
        
        metadata = {
            "timestamp": timestamp,
            "frame_count": frame_count
        }
        
        # åˆ†å‘åˆ°ç–²åŠ³åº¦æ¨¡å‹(ä½¿ç”¨å®Œæ•´çš„å¤šæ¨¡æ€æ•°æ®)
        if "fatigue" in self.integrated_models or "fatigue" in self.remote_clients:
            inference_data = {
                "elapsed_time": elapsed_time
            }
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©æ•°æ®æ ¼å¼(ä¼˜å…ˆä½¿ç”¨å†…å­˜æ¨¡å¼)
            if memory_mode:
                inference_data.update({
                    "memory_mode": True,
                    "rgb_frames_memory": rgb_frames_memory,
                    "depth_frames_memory": depth_frames_memory,
                    "eyetrack_memory": eyetrack_memory,
                })
            elif file_mode:
                inference_data.update({
                    "file_mode": True,
                    "rgb_video_path": rgb_video_path,
                    "depth_video_path": depth_video_path,
                    "eyetrack_json_path": eyetrack_json_path,
                })
            else:
                inference_data.update({
                    "rgb_frames": rgb_frames_b64,
                    "depth_frames": depth_frames_b64,
                    "eyetrack_samples": eyetrack_samples,
                })
            
            self._submit_inference("fatigue", inference_data, metadata)
    
    def _on_emotion_request(self, event: Event) -> None:
        """å¤„ç†æƒ…ç»ªåˆ†æè¯·æ±‚"""
        payload = event.payload or {}
        request_id = payload.get("request_id")
        audio_paths = payload.get("audio_paths", [])
        video_paths = payload.get("video_paths", [])
        text_data = payload.get("text_data", [])
        
        if not audio_paths and not video_paths:
            self.logger.warning("æƒ…ç»ªåˆ†æè¯·æ±‚ç¼ºå°‘éŸ³è§†é¢‘æ•°æ®")
            return
        
        # åˆ†å‘åˆ°æƒ…ç»ªæ¨¡å‹
        if "emotion" in self.integrated_models or "emotion" in self.remote_clients:
            # æå–æ–‡æœ¬æ•°æ®ï¼ˆå­—æ®µåæ˜¯ recognized_textï¼‰
            text_list = []
            for item in text_data:
                if isinstance(item, dict):
                    text = item.get("recognized_text", "")
                    text_list.append(text)
            
            # è®°å½•æå–çš„æ–‡æœ¬æ•°æ®
            if text_data:
                total_chars = sum(len(t) for t in text_list)
                self.logger.info(f"\n{'='*60}")
                self.logger.info(f"ğŸ“ è¯­éŸ³è¯†åˆ«æ–‡æœ¬æå–")
                self.logger.info(f"{'='*60}")
                self.logger.info(f"æ ·æœ¬æ•°é‡: {len(text_list)}")
                self.logger.info(f"æ€»å­—ç¬¦æ•°: {total_chars}")
                self.logger.info(f"-" * 60)
                for i, text in enumerate(text_list, 1):
                    self.logger.info(f"ç¬¬{i}é¢˜: {text}")
                self.logger.info(f"{'='*60}\n")
            else:
                self.logger.warning("âš ï¸  æœªæå–åˆ°è¯­éŸ³è¯†åˆ«æ–‡æœ¬")
            
            # ä½¿ç”¨å¤šæ ·æœ¬æ¨¡å¼è¿›è¡Œæ¨ç†
            num_samples = min(len(video_paths), len(audio_paths))
            if num_samples == 0:
                self.logger.warning("æ²¡æœ‰å¯ç”¨çš„éŸ³è§†é¢‘æ–‡ä»¶")
                return
            
            # æ„å»ºå¤šæ ·æœ¬æ¨ç†æ•°æ®
            inference_data = {
                "multi_sample_mode": True,  # æ–°å¢å¤šæ ·æœ¬æ¨¡å¼
                "video_paths": video_paths[:num_samples],
                "audio_paths": audio_paths[:num_samples],
                "text_list": text_list[:num_samples]  # æŒ‰æ ·æœ¬é¡ºåºçš„æ–‡æœ¬åˆ—è¡¨
            }
            
            metadata = {
                "request_id": request_id,
                "timestamp": payload.get("timestamp")
            }
            
            self._submit_inference("emotion", inference_data, metadata)
    
    def _on_eeg_request(self, event: Event) -> None:
        """å¤„ç†EEGè„‘è´Ÿè·åˆ†æè¯·æ±‚"""
        payload = event.payload or {}
        request_id = payload.get("request_id")
        eeg_signal = payload.get("eeg_signal")
        sampling_rate = payload.get("sampling_rate", 250)
        subject_id = payload.get("subject_id", "unknown")
        memory_mode = payload.get("memory_mode", True)
        
        if eeg_signal is None:
            self.logger.warning("EEGåˆ†æè¯·æ±‚ç¼ºå°‘ä¿¡å·æ•°æ®")
            return
        
        # åˆ†å‘åˆ°EEGæ¨¡å‹
        if "eeg" in self.integrated_models or "eeg" in self.remote_clients:
            # æ„å»ºæ¨ç†æ•°æ®
            inference_data = {
                "memory_mode": memory_mode,
                "eeg_signal": eeg_signal,
                "sampling_rate": sampling_rate,
                "subject_id": subject_id
            }
            
            metadata = {
                "request_id": request_id,
                "timestamp": payload.get("timestamp")
            }
            
            self._submit_inference("eeg", inference_data, metadata)
    
    def _submit_inference(
        self,
        model_type: str,
        data: Dict[str, Any],
        metadata: Dict[str, Any]
    ) -> None:
        """æäº¤æ¨ç†ä»»åŠ¡ï¼ˆå¼‚æ­¥ï¼‰"""
        def _infer():
            try:
                # é€‰æ‹©æ¨ç†æ–¹å¼
                if model_type in self.integrated_models:
                    result = self._infer_integrated(model_type, data)
                elif model_type in self.remote_clients:
                    result = self._infer_remote(model_type, data)
                else:
                    self.logger.warning(f"æ¨¡å‹æœªåŠ è½½: {model_type}")
                    return
                
                # å‘å¸ƒç»“æœ
                if result:
                    self._publish_result(model_type, result, metadata)
                    
            except Exception as e:
                self.logger.error(f"æ¨ç†ä»»åŠ¡å¤±è´¥ ({model_type}): {e}", exc_info=True)
        
        self._executor.submit(_infer)
    
    def _infer_integrated(
        self,
        model_type: str,
        data: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨é›†æˆæ¨¡å‹æ¨ç†"""
        model = self.integrated_models[model_type]
        
        try:
            self.logger.info(f"ğŸ”„ å¼€å§‹é›†æˆæ¨¡å‹æ¨ç†: {model_type}")
            result = model.infer(data)
            
            # è¾“å‡ºæ¨ç†ç»“æœå…³é”®ä¿¡æ¯
            if result and result.get("status") == "success":
                predictions = result.get("predictions", result)
                inference_mode = result.get("inference_mode", "unknown")
                inference_time = result.get("inference_time_ms", 0)
                if model_type == "fatigue":
                    fatigue_score = predictions.get("fatigue_score", 0)
                    prediction_class = predictions.get("prediction_class", 0)
                    # å‹ç¼©è¾“å‡º: ç–²åŠ³åº¦å•è¡Œæ˜¾ç¤º
                    self.logger.info(f"âœ…ç–²åŠ³åº¦ {fatigue_score:.1f} [C{prediction_class}] {inference_time:.0f}ms")
                elif model_type == "emotion":
                    emotion_score = predictions.get("emotion_score", 0)
                    inference_time = result.get("inference_time_ms", 0)
                    self.logger.info(f"âœ…æƒ…ç»ª {emotion_score:.1f} {inference_time:.0f}ms")
                elif model_type == "eeg":
                    brain_load_score = predictions.get("brain_load_score", 0)
                    state = predictions.get("state", "unknown")
                    num_windows = predictions.get("num_windows", 0)
                    # å‹ç¼©è¾“å‡º: è„‘è´Ÿè·å•è¡Œæ˜¾ç¤º
                    self.logger.info(f"âœ…è„‘è´Ÿè· {brain_load_score:.1f} [{state[:3]}] {num_windows}win {inference_time:.0f}ms")
                else:
                    self.logger.info(f"âœ… {model_type} æ¨ç†å®Œæˆ")
            else:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯") if result else "è¿”å›ç»“æœä¸ºç©º"
                self.logger.warning(f"âš ï¸  {model_type} æ¨ç†å¤±è´¥: {error_msg}")
                # æ¨ç†å¤±è´¥æ—¶è¿”å›Noneï¼Œä¸å‘å¸ƒç»“æœ
                return None
            
            return result
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œé¿å…å½±å“ç³»ç»Ÿè¿è¡Œ
            error_msg = str(e)
            self.logger.error(f"é›†æˆæ¨¡å‹æ¨ç†å¼‚å¸¸ ({model_type}): {error_msg}")
            
            # å¯¹äºç‰¹å®šçš„é”™è¯¯ï¼Œæä¾›æ›´å‹å¥½çš„æç¤º
            if "Invalid computed output size" in error_msg:
                self.logger.debug(
                    f"æç¤º: {model_type}æ¨¡å‹è¾“å…¥æ•°æ®ä¸è¶³ï¼Œ"
                    "å¯èƒ½æ˜¯å› ä¸ºæ‘„åƒå¤´æœªå¼€å¯æˆ–é‡‡é›†å¸§æ•°è¿‡å°‘"
                )
            
            return None
    
    def _infer_remote(
        self,
        model_type: str,
        data: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨è¿œç¨‹å®¢æˆ·ç«¯æ¨ç†"""
        client = self.remote_clients[model_type]
        
        if not client.is_healthy():
            self.logger.warning(f"è¿œç¨‹æ¨¡å‹æœªè¿æ¥: {model_type}")
            return None
        
        try:
            # å‹ç¼©æ—¥å¿—: ä¸å•ç‹¬è¾“å‡º"å‘é€è¯·æ±‚",åªè¾“å‡ºç»“æœ
            future = client.send_inference_request(data, timeout=10.0)
            result = future.result(timeout=10.0)
            
            # è¾“å‡ºæ¨ç†ç»“æœå…³é”®ä¿¡æ¯(å‹ç¼©å•è¡Œ)
            if result and result.get("status") == "success":
                predictions = result.get("predictions", result)
                if model_type == "fatigue":
                    fatigue_score = predictions.get("fatigue_score", 0)
                    self.logger.info(f"âœ…è¿œç¨‹ç–²åŠ³åº¦ {fatigue_score:.1f}")
                elif model_type == "emotion":
                    emotion_score = predictions.get("emotion_score", 0)
                    self.logger.info(f"âœ…è¿œç¨‹æƒ…ç»ª {emotion_score:.1f}")
                else:
                    self.logger.info(f"âœ…è¿œç¨‹{model_type}å®Œæˆ")
            else:
                self.logger.warning(f"âš ï¸è¿œç¨‹{model_type}å¼‚å¸¸")
            
            return result
        except Exception as e:
            self.logger.error(f"è¿œç¨‹æ¨¡å‹æ¨ç†å¤±è´¥ ({model_type}): {e}", exc_info=True)
            return None
    
    def _publish_result(
        self,
        model_type: str,
        result: Dict[str, Any],
        metadata: Dict[str, Any]
    ) -> None:
        """å‘å¸ƒæ¨ç†ç»“æœåˆ°äº‹ä»¶æ€»çº¿"""
        if result.get("status") != "success":
            error = result.get("error", "Unknown error")
            self.logger.error(f"{model_type} æ¨ç†å¤±è´¥: {error}")
            return
        
        # æå–é¢„æµ‹ç»“æœï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹ï¼‰
        predictions = {}
        if model_type == "fatigue":
            predictions = {
                "fatigue_score": result.get("fatigue_score", 0),
                "prediction_class": result.get("prediction_class", 0),
                "inference_time_ms": result.get("inference_time_ms", 0),
                "inference_mode": result.get("inference_mode", "unknown")
            }
        elif model_type == "emotion":
            predictions = {
                "emotion_score": result.get("emotion_score", 0),
                "prediction": result.get("prediction", 0),
                "probabilities": result.get("probabilities", []),
                "inference_time_ms": result.get("inference_time_ms", 0),
                "inference_mode": result.get("inference_mode", "file")
            }
        elif model_type == "eeg":
            predictions = {
                "brain_load_score": result.get("brain_load_score", 0),
                "state": result.get("state", "unknown"),
                "num_windows": result.get("num_windows", 0),
                "inference_time_ms": result.get("inference_time_ms", 0),
                "inference_mode": result.get("inference_mode", "memory")
            }
        
        # å‘å¸ƒåˆ°äº‹ä»¶æ€»çº¿
        self.bus.publish(Event(
            topic=EventTopic.DETECTION_RESULT,
            payload={
                "detector": f"model_{model_type}",
                "status": "detected",
                "label": model_type,
                "predictions": predictions,
                "request_id": metadata.get("request_id"),  # ç”¨äºæƒ…ç»ªåˆ†æè¯·æ±‚åŒ¹é…
                "timestamp": metadata.get("timestamp"),
                "frame_count": metadata.get("frame_count")
            }
        ))
        
        self.logger.debug(f"âœ… {model_type} æ¨ç†å®Œæˆ: {predictions}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            "running": self._running,
            "integrated_models": list(self.integrated_models.keys()),
            "remote_clients": list(self.remote_clients.keys()),
            "total": len(self.integrated_models) + len(self.remote_clients)
        }


__all__ = ["UnifiedInferenceService"]

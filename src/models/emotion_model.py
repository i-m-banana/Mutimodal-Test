"""æƒ…ç»ªæ¨¡å‹ - ç›´æ¥é›†æˆç‰ˆæœ¬

é›†æˆ emotion_fatigue_infer/emotion ä¸­çš„æ¨ç†ä»£ç 
"""

import base64
import io
import os
import sys
import tempfile
from pathlib import Path
from typing import Any, Dict, List

import numpy as np

from .base_inference_model import BaseInferenceModel

# æ·»åŠ  emotion_fatigue_infer è·¯å¾„
_EMOTION_FATIGUE_PATH = Path(__file__).parent / "emotion_fatigue_infer"
_EMOTION_PATH = _EMOTION_FATIGUE_PATH / "emotion"
sys.path.insert(0, str(_EMOTION_FATIGUE_PATH))
sys.path.insert(0, str(_EMOTION_PATH))

try:
    import torch
    import cv2
    import soundfile as sf
    from transformers import VivitImageProcessor, Wav2Vec2Processor, AutoTokenizer, AutoModel, Wav2Vec2Model
    from emotion.inference_standalone_all import (
        SimpleMultimodalClassifier,
        extract_vision_feature,
        extract_audio_feature,
        extract_text_feature
    )
    HAS_DEPS = True
except ImportError as e:
    HAS_DEPS = False
    _import_error = e


class EmotionModel(BaseInferenceModel):
    """æƒ…ç»ªæ¨¡å‹ï¼ˆé›†æˆç‰ˆæœ¬ï¼‰
    
    ç›´æ¥åœ¨åç«¯è¿›ç¨‹ä¸­è¿è¡Œï¼Œæ— éœ€ç‹¬ç«‹è¿›ç¨‹
    """
    
    def initialize(self) -> None:
        """åˆå§‹åŒ–æƒ…ç»ªæ¨¡å‹"""
        if not HAS_DEPS:
            raise RuntimeError(f"æ— æ³•åŠ è½½ä¾èµ–: {_import_error}")
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„ - ä»æ ¹ç›®å½•çš„models_dataæ–‡ä»¶å¤¹åŠ è½½
        project_root = Path(__file__).parent.parent.parent
        models_dir = project_root / "models_data" / "emotion_models"
        model_path = models_dir / "best_model.pt"
        # é¢„è®­ç»ƒæ¨¡å‹ä¹Ÿåœ¨models_data
        pretrained_models_dir = project_root / "models_data" / "emotion_pretrained_models"
        
        if not model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # è®¾ç½®è®¾å¤‡
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½é¢„å¤„ç†å™¨
        self.logger.info("åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        self.vision_processor = VivitImageProcessor.from_pretrained(str(pretrained_models_dir / "TIMESFORMER"))
        self.audio_processor = Wav2Vec2Processor.from_pretrained(str(pretrained_models_dir / "WAV2VEC2"))
        self.text_tokenizer = AutoTokenizer.from_pretrained(str(pretrained_models_dir / "ROBBERTA"))
        
        # åŠ è½½ç‰¹å¾æå–æ¨¡å‹
        self.vision_model = AutoModel.from_pretrained(
            str(pretrained_models_dir / "TIMESFORMER"), 
            ignore_mismatched_sizes=True
        ).to(self.device)
        self.vision_model.eval()
        
        self.audio_model = Wav2Vec2Model.from_pretrained(
            str(pretrained_models_dir / "WAV2VEC2"),
            ignore_mismatched_sizes=True
        ).to(self.device)
        self.audio_model.eval()
        
        self.text_model = AutoModel.from_pretrained(
            str(pretrained_models_dir / "ROBBERTA"),
            ignore_mismatched_sizes=True
        ).to(self.device)
        self.text_model.eval()        # é¢„çƒ­å¹¶è·å–ç‰¹å¾ç»´åº¦
        self.logger.info("é¢„çƒ­æ¨¡å‹...")
        dummy_video = np.zeros((224, 224, 3), dtype=np.uint8)
        dummy_audio = np.zeros(16000, dtype=np.float32)
        dummy_text = "test"
        
        with tempfile.NamedTemporaryFile(suffix='.avi', delete=False) as tmp_video:
            tmp_video_path = tmp_video.name
            out = cv2.VideoWriter(tmp_video_path, cv2.VideoWriter_fourcc(*'XVID'), 30, (224, 224))
            for _ in range(8):
                out.write(dummy_video)
            out.release()
        
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_audio:
            tmp_audio_path = tmp_audio.name
            sf.write(tmp_audio_path, dummy_audio, 16000)
        
        try:
            vision_feat = extract_vision_feature(tmp_video_path, self.vision_processor, self.vision_model, self.device)
            audio_feat = extract_audio_feature(tmp_audio_path, self.audio_processor, self.audio_model, self.device)
            text_feat = extract_text_feature(dummy_text, self.text_tokenizer, self.text_model, self.device)
            
            # åˆå§‹åŒ–åˆ†ç±»å™¨
            self.model = SimpleMultimodalClassifier(
                vision_feat_dim=vision_feat.shape[-1],
                audio_feat_dim=audio_feat.shape[-1],
                text_feat_dim=text_feat.shape[-1],
                hidden_dim=512,
                num_classes=2,
            ).to(self.device)
            
            # åŠ è½½æƒé‡
            self.model.load_state_dict(torch.load(str(model_path), map_location=self.device))
            self.model.eval()
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_video_path):
                os.remove(tmp_video_path)
            if os.path.exists(tmp_audio_path):
                os.remove(tmp_audio_path)
        
        self.logger.info("æƒ…ç»ªæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def infer(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨ç†
        
        æ”¯æŒä¸‰ç§è¾“å…¥æ¨¡å¼ï¼š
        1. base64 æ•°æ®æ¨¡å¼ï¼ˆåŸæœ‰ï¼‰ï¼š
           - samples: List[Dict] - åŒ…å«video_b64, audio_b64, textçš„æ ·æœ¬åˆ—è¡¨
           
        2. æ–‡ä»¶è·¯å¾„æ¨¡å¼ï¼ˆå•æ ·æœ¬ï¼‰ï¼š
           - file_mode: bool = True
           - video_path: str - è§†é¢‘æ–‡ä»¶è·¯å¾„
           - audio_path: str - éŸ³é¢‘æ–‡ä»¶è·¯å¾„
           - text: str = "" - æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‰ï¼‰
           
        3. å¤šæ ·æœ¬æ–‡ä»¶æ¨¡å¼ï¼ˆæ–°å¢ï¼‰ï¼š
           - multi_sample_mode: bool = True
           - video_paths: List[str] - è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
           - audio_paths: List[str] - éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
           - text_list: List[str] - æ–‡æœ¬å†…å®¹åˆ—è¡¨
        
        Args:
            data: è¾“å…¥æ•°æ®å­—å…¸
        
        Returns:
            æ¨ç†ç»“æœ:
                - emotion_score: æƒ…ç»ªåˆ†æ•° (0-100)
                - sample_results: æ¯ä¸ªæ ·æœ¬çš„ç»“æœï¼ˆå¦‚æœæ˜¯å¤šæ ·æœ¬ï¼‰
        """
        # æ£€æŸ¥æ¨ç†æ¨¡å¼
        if data.get("multi_sample_mode") == True:
            return self._infer_from_multiple_files(data)
        elif data.get("file_mode") == True:
            return self._infer_from_files(data)
        else:
            return self._infer_from_base64(data)
    
    def _infer_from_multiple_files(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ä»å¤šä¸ªæ–‡ä»¶è·¯å¾„è¯»å–æ•°æ®å¹¶æ¨ç†ï¼ˆå¤šæ ·æœ¬æ¨¡å¼ï¼‰"""
        import time
        from pathlib import Path
        
        start_time = time.time()
        
        video_paths = data.get("video_paths", [])
        audio_paths = data.get("audio_paths", [])
        text_list = data.get("text_list", [])
        
        if not video_paths or not audio_paths:
            return {
                "status": "error",
                "error": "ç¼ºå°‘å¿…éœ€çš„è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨",
                "emotion_score": 0.0
            }
        
        num_samples = min(len(video_paths), len(audio_paths))
        if num_samples == 0:
            return {
                "status": "error",
                "error": "æ ·æœ¬æ•°é‡ä¸º0",
                "emotion_score": 0.0
            }
        
        try:
            logits_list = []
            sample_results = []
            
            for idx in range(num_samples):
                video_path = video_paths[idx]
                audio_path = audio_paths[idx]
                text = text_list[idx] if idx < len(text_list) else ""
                
                # éªŒè¯æ–‡ä»¶å­˜åœ¨
                if not Path(video_path).exists():
                    self.logger.warning(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                    continue
                
                if not Path(audio_path).exists():
                    self.logger.warning(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
                    continue
                
                # æå–ç‰¹å¾
                sample_start = time.time()
                v_feat = extract_vision_feature(str(video_path), self.vision_processor, self.vision_model, self.device)
                a_feat = extract_audio_feature(str(audio_path), self.audio_processor, self.audio_model, self.device)
                t_feat = extract_text_feature(text, self.text_tokenizer, self.text_model, self.device)
                
                # æ¨ç†
                with torch.no_grad():
                    logits = self.model(v_feat.unsqueeze(0), a_feat.unsqueeze(0), t_feat.unsqueeze(0))
                    probs = torch.softmax(logits, dim=1)
                    logits_list.append(logits.squeeze(0).cpu().numpy())
                    pred = torch.argmax(logits, dim=1).item()
                    prob_values = probs.squeeze(0).cpu().numpy()
                
                sample_time = (time.time() - sample_start) * 1000
                
                sample_results.append({
                    "sample_index": idx + 1,
                    "prediction": pred,
                    "probabilities": prob_values.tolist(),
                    "text": text,
                    "video_file": Path(video_path).name,
                    "audio_file": Path(audio_path).name
                })
            
            if not logits_list:
                return {
                    "status": "error",
                    "error": "æ‰€æœ‰æ ·æœ¬æ¨ç†å¤±è´¥",
                    "emotion_score": 0.0
                }
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼ˆä¸base64æ¨¡å¼ç›¸åŒçš„é€»è¾‘ï¼‰
            logits_arr = np.array(logits_list)
            probs = torch.softmax(torch.tensor(logits_arr), dim=1).numpy()
            pos_probs = probs[:, 1]
            
            min_prob, max_prob = pos_probs.min(), pos_probs.max()
            if max_prob - min_prob < 1e-6:
                scores = np.full_like(pos_probs, 50.0)
            else:
                scores = (pos_probs - min_prob) / (max_prob - min_prob) * 100
            
            final_score = float(np.mean(scores))
            
            inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # å•è¡Œè¾“å‡ºæœ€ç»ˆç»“æœ
            emotion_label = "ç§¯æğŸ˜Š" if final_score >= 50 else "æ¶ˆæğŸ˜”"
            self.logger.info(
                f"ğŸ˜Š æƒ…ç»ª: {round(final_score, 2)} ({emotion_label}, "
                f"{len(logits_list)}æ ·æœ¬, {round(inference_time, 1)}ms)"
            )
            
            return {
                "status": "success",
                "emotion_score": round(final_score, 2),
                "sample_scores": scores.tolist(),
                "sample_results": sample_results,
                "num_samples": len(logits_list),
                "inference_time_ms": round(inference_time, 1),
                "inference_mode": "multi_file"
            }
            
        except Exception as e:
            self.logger.error(f"å¤šæ ·æœ¬æ¨ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "emotion_score": 0.0
            }
    
    def _infer_from_files(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ä»æ–‡ä»¶è·¯å¾„è¯»å–æ•°æ®å¹¶æ¨ç†ï¼ˆå•æ ·æœ¬æ¨¡å¼ï¼‰"""
        import time
        from pathlib import Path
        
        start_time = time.time()
        
        video_path = data.get("video_path")
        audio_path = data.get("audio_path")
        text = data.get("text", "")
        
        if not video_path or not audio_path:
            return {
                "status": "error",
                "error": "ç¼ºå°‘å¿…éœ€çš„è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶è·¯å¾„",
                "emotion_score": 0.0
            }
        
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not Path(video_path).exists():
            return {
                "status": "error",
                "error": f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}",
                "emotion_score": 0.0
            }
        
        if not Path(audio_path).exists():
            return {
                "status": "error",
                "error": f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}",
                "emotion_score": 0.0
            }
        
        try:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"ğŸ¬ å•æ ·æœ¬æƒ…ç»ªåˆ†æ")
            self.logger.info(f"{'='*60}")
            self.logger.info(f"ğŸ“¹ è§†é¢‘: {Path(video_path).name}")
            self.logger.info(f"ğŸµ éŸ³é¢‘: {Path(audio_path).name}")
            self.logger.info(f"ğŸ“ æ–‡æœ¬: '{text}'")
            
            # æå–ç‰¹å¾
            v_feat = extract_vision_feature(str(video_path), self.vision_processor, self.vision_model, self.device)
            a_feat = extract_audio_feature(str(audio_path), self.audio_processor, self.audio_model, self.device)
            t_feat = extract_text_feature(text, self.text_tokenizer, self.text_model, self.device)
            
            # æ¨ç†
            with torch.no_grad():
                logits = self.model(v_feat.unsqueeze(0), a_feat.unsqueeze(0), t_feat.unsqueeze(0))
                probs = torch.softmax(logits, dim=1)
                pred = torch.argmax(logits, dim=1).item()
                prob_values = probs.squeeze(0).cpu().numpy()
            
            # è®¡ç®—åˆ†æ•°ï¼ˆ0-100ï¼‰
            score = float(prob_values[1] * 100) if len(prob_values) > 1 else float(prob_values[0] * 100)
            
            inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è®°å½•æ¨ç†ç»“æœ
            emotion_label = "ç§¯æğŸ˜Š" if pred == 1 else "æ¶ˆæğŸ˜”"
            confidence = float(prob_values[pred]) * 100
            self.logger.info(f"âœ… æ¨ç†å®Œæˆ:")
            self.logger.info(f"   åˆ†æ•°: {round(score, 2)}")
            self.logger.info(f"   ç±»åˆ«: {pred} ({emotion_label})")
            self.logger.info(f"   ç½®ä¿¡åº¦: {confidence:.1f}%")
            self.logger.info(f"   è€—æ—¶: {round(inference_time, 1)}ms")
            self.logger.info(f"{'='*60}\n")
            
            return {
                "status": "success",
                "emotion_score": round(score, 2),
                "prediction": pred,
                "probabilities": prob_values.tolist(),
                "text_input": text,
                "inference_time_ms": round(inference_time, 1),
                "inference_mode": "file"
            }
            
        except Exception as e:
            self.logger.error(f"ä»æ–‡ä»¶æ¨ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "emotion_score": 0.0
            }
    
    def _infer_from_base64(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ä»base64æ•°æ®æ¨ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        import time
        start_time = time.time()
        
        samples = data.get("samples", [])
        
        if not samples:
            return {
                "status": "error",
                "error": "ç¼ºå°‘æ ·æœ¬æ•°æ®",
                "emotion_score": 0.0
            }
        
        temp_files = []
        try:
            logits_list = []
            sample_results = []
            
            for idx, sample in enumerate(samples):
                video_b64 = sample.get("video_b64", "")
                audio_b64 = sample.get("audio_b64", "")
                text = sample.get("text", "")
                question_index = sample.get("question_index", idx + 1)
                
                # è§£ç å¹¶ä¿å­˜è§†é¢‘
                video_bytes = base64.b64decode(video_b64)
                tmp_video = tempfile.NamedTemporaryFile(suffix='.avi', delete=False)
                tmp_video.write(video_bytes)
                tmp_video.close()
                temp_files.append(tmp_video.name)
                
                # è§£ç å¹¶ä¿å­˜éŸ³é¢‘
                audio_bytes = base64.b64decode(audio_b64)
                tmp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                tmp_audio.write(audio_bytes)
                tmp_audio.close()
                temp_files.append(tmp_audio.name)
                
                # æå–ç‰¹å¾
                v_feat = extract_vision_feature(tmp_video.name, self.vision_processor, self.vision_model, self.device)
                a_feat = extract_audio_feature(tmp_audio.name, self.audio_processor, self.audio_model, self.device)
                t_feat = extract_text_feature(text, self.text_tokenizer, self.text_model, self.device)
                
                # æ¨ç†
                with torch.no_grad():
                    logits = self.model(v_feat.unsqueeze(0), a_feat.unsqueeze(0), t_feat.unsqueeze(0))
                    probs = torch.softmax(logits, dim=1)
                    logits_list.append(logits.squeeze(0).cpu().numpy())
                    pred = torch.argmax(logits, dim=1).item()
                    prob_values = probs.squeeze(0).cpu().numpy()
                
                sample_results.append({
                    "question_index": question_index,
                    "prediction": pred,
                    "probabilities": prob_values.tolist()
                })
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            logits_arr = np.array(logits_list)
            probs = torch.softmax(torch.tensor(logits_arr), dim=1).numpy()
            pos_probs = probs[:, 1]
            
            min_prob, max_prob = pos_probs.min(), pos_probs.max()
            if max_prob - min_prob < 1e-6:
                scores = np.full_like(pos_probs, 50.0)
            else:
                scores = (pos_probs - min_prob) / (max_prob - min_prob) * 100
            
            final_score = float(np.mean(scores))
            
            inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            self.logger.info(
                f"âœ… æƒ…ç»ªæ¨ç†å®Œæˆ(Base64): åˆ†æ•°={round(final_score, 2)}, æ ·æœ¬æ•°={len(samples)}, è€—æ—¶={round(inference_time, 1)}ms"
            )
            
            return {
                "status": "success",
                "emotion_score": round(final_score, 2),
                "sample_scores": scores.tolist(),
                "sample_results": sample_results,
                "num_samples": len(samples),
                "inference_time_ms": round(inference_time, 1),
                "inference_mode": "base64"
            }
            
        except Exception as e:
            self.logger.error(f"æƒ…ç»ªæ¨ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "emotion_score": 0.0
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except:
                        pass
    
    def cleanup(self) -> None:
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'vision_model'):
            del self.vision_model
        if hasattr(self, 'audio_model'):
            del self.audio_model
        if hasattr(self, 'text_model'):
            del self.text_model
        
        if HAS_DEPS and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        import gc
        gc.collect()


__all__ = ["EmotionModel"]

"""ç–²åŠ³åº¦æ¨¡å‹ - ç›´æ¥é›†æˆç‰ˆæœ¬

ç›´æ¥é›†æˆ emotion_fatigue_infer/fatigue ä¸­çš„æ¨ç†ä»£ç 
"""

import base64
import io
import sys
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
from PIL import Image

from .base_inference_model import BaseInferenceModel

# æ·»åŠ  emotion_fatigue_infer è·¯å¾„
_EMOTION_FATIGUE_PATH = Path(__file__).parent.parent.parent / "model_backends" / "emotion_fatigue_infer"
_FATIGUE_PATH = _EMOTION_FATIGUE_PATH / "fatigue"
sys.path.insert(0, str(_EMOTION_FATIGUE_PATH))
sys.path.insert(0, str(_FATIGUE_PATH))

try:
    import torch
    from fatigue.infer_multimodal import (
        FatigueFaceOnlyCNN,
        extract_eye_features_from_samples,
        extract_face_features_from_frames
    )
    HAS_TORCH = True
except ImportError as e:
    HAS_TORCH = False
    _import_error = e


class FatigueModel(BaseInferenceModel):
    """ç–²åŠ³åº¦æ¨¡å‹ï¼ˆé›†æˆç‰ˆæœ¬ï¼‰
    
    ç›´æ¥åœ¨åç«¯è¿›ç¨‹ä¸­è¿è¡Œï¼Œæ— éœ€ç‹¬ç«‹è¿›ç¨‹
    """
    
    def initialize(self) -> None:
        """åˆå§‹åŒ–ç–²åŠ³åº¦æ¨¡å‹"""
        if not HAS_TORCH:
            raise RuntimeError(f"æ— æ³•åŠ è½½PyTorchä¾èµ–: {_import_error}")
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„ - ä»æ ¹ç›®å½•çš„models_dataæ–‡ä»¶å¤¹åŠ è½½
        project_root = Path(__file__).parent.parent.parent
        models_dir = project_root / "models_data" / "fatigue_models"
        model_path = models_dir / "fatigue_best_model.pt"
        if not model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # è®¾ç½®è®¾å¤‡
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        self.model = FatigueFaceOnlyCNN().to(self.device)
        self.model.load_state_dict(torch.load(str(model_path), map_location=self.device))
        self.model.eval()
        
        self.logger.info("ç–²åŠ³åº¦æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def infer(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨ç†
        
        æ”¯æŒä¸‰ç§è¾“å…¥æ¨¡å¼ï¼š
        1. å†…å­˜æ¨¡å¼ï¼ˆæ¨èï¼Œé›¶I/Oå¼€é”€ï¼‰ï¼š
           - memory_mode: bool = True
           - rgb_frames_memory: List[np.ndarray] - RGBå›¾åƒnumpyæ•°ç»„
           - depth_frames_memory: List[np.ndarray] - æ·±åº¦å›¾åƒnumpyæ•°ç»„
           - eyetrack_memory: List[List[float]] - çœ¼åŠ¨ç‰¹å¾æ•°æ®
           
        2. æ–‡ä»¶è·¯å¾„æ¨¡å¼ï¼ˆå­˜æ¡£å¤‡ä»½ï¼‰ï¼š
           - file_mode: bool = True
           - rgb_video_path: str - RGBè§†é¢‘æ–‡ä»¶è·¯å¾„
           - depth_video_path: str - æ·±åº¦è§†é¢‘æ–‡ä»¶è·¯å¾„
           - eyetrack_json_path: str - çœ¼åŠ¨æ•°æ®JSONæ–‡ä»¶è·¯å¾„
           - max_frames: int = 30 - æœ€å¤§è¯»å–å¸§æ•°
           
        3. base64 æ•°æ®æ¨¡å¼ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰ï¼š
           - rgb_frames: List[str] - base64ç¼–ç çš„RGBå›¾åƒ
           - depth_frames: List[str] - base64ç¼–ç çš„æ·±åº¦å›¾åƒ
           - eyetrack_samples: List[Dict] - çœ¼åŠ¨æ•°æ®
        
        Args:
            data: è¾“å…¥æ•°æ®å­—å…¸
        
        Returns:
            æ¨ç†ç»“æœ:
                - fatigue_score: ç–²åŠ³åº¦åˆ†æ•° (0-100)
                - prediction_class: é¢„æµ‹ç±»åˆ«
        """
        # ä¼˜å…ˆä½¿ç”¨å†…å­˜æ¨¡å¼(é¿å…æ–‡ä»¶I/O)
        if data.get("memory_mode") == True:
            return self._infer_from_memory(data)
        elif data.get("file_mode") == True:
            return self._infer_from_files(data)
        else:
            return self._infer_from_base64(data)
    
    def _infer_from_memory(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ä»å†…å­˜ä¸­çš„numpyæ•°ç»„ç›´æ¥æ¨ç†ï¼ˆé›¶I/Oå¼€é”€ï¼‰"""
        import time
        start_time = time.time()
        
        rgb_frames = data.get("rgb_frames_memory", [])
        depth_frames = data.get("depth_frames_memory", [])
        eyetrack_samples = data.get("eyetrack_memory", [])
        
        if not rgb_frames or not depth_frames:
            return {
                "status": "error",
                "error": "ç¼ºå°‘å¿…éœ€çš„å›¾åƒæ•°æ®",
                "fatigue_score": 0.0,
                "prediction_class": 0
            }
        
        try:
            # ç›´æ¥ä½¿ç”¨numpyæ•°ç»„,æ— éœ€è§£ç æˆ–I/O
            frames = min(len(rgb_frames), len(depth_frames))
            
            # æå–ç‰¹å¾
            face_feat = extract_face_features_from_frames(
                rgb_frames, depth_frames, frames=frames
            ).to(self.device)
            
            eye_feat = extract_eye_features_from_samples(eyetrack_samples).to(self.device)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output = self.model(eye_feat, face_feat)
                probs = output.cpu().numpy()[0]
                num_classes = output.shape[1]
                
                scores = np.linspace(0, 100, num_classes)
                score = float(np.dot(probs, scores))
                pred = int(np.argmax(probs))
            
            inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # å•è¡Œè¾“å‡ºæ¨ç†ç»“æœ
            fatigue_level = "æ­£å¸¸ğŸ˜Š" if score < 30 else "è½»åº¦ç–²åŠ³ğŸ˜" if score < 60 else "é‡åº¦ç–²åŠ³ğŸ˜´"
            self.logger.info(
                f"ğŸ˜´ ç–²åŠ³åº¦: {round(score, 2)} ({fatigue_level}, "
                f"RGB{len(rgb_frames)}+æ·±åº¦{len(depth_frames)}+çœ¼åŠ¨{len(eyetrack_samples)}, {round(inference_time, 1)}ms)"
            )
            
            return {
                "status": "success",
                "fatigue_score": round(score, 2),
                "prediction_class": pred,
                "num_rgb_frames": len(rgb_frames),
                "num_depth_frames": len(depth_frames),
                "num_eyetrack_samples": len(eyetrack_samples),
                "inference_mode": "memory",
                "inference_time_ms": round(inference_time, 1)
            }
            
        except Exception as e:
            self.logger.error(f"å†…å­˜æ¨ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "fatigue_score": 0.0,
                "prediction_class": 0
            }
    
    def _infer_from_files(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ä»æ–‡ä»¶è·¯å¾„è¯»å–æ•°æ®å¹¶æ¨ç†"""
        import cv2
        import json
        import time
        from pathlib import Path
        
        start_time = time.time()
        
        rgb_video_path = data.get("rgb_video_path")
        depth_video_path = data.get("depth_video_path")
        eyetrack_json_path = data.get("eyetrack_json_path")
        max_frames = data.get("max_frames", 30)
        
        if not rgb_video_path or not depth_video_path:
            return {
                "status": "error",
                "error": "ç¼ºå°‘å¿…éœ€çš„è§†é¢‘æ–‡ä»¶è·¯å¾„",
                "fatigue_score": 0.0,
                "prediction_class": 0
            }
        
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not Path(rgb_video_path).exists():
            return {
                "status": "error",
                "error": f"RGBè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {rgb_video_path}",
                "fatigue_score": 0.0,
                "prediction_class": 0
            }
        
        if not Path(depth_video_path).exists():
            return {
                "status": "error",
                "error": f"æ·±åº¦è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {depth_video_path}",
                "fatigue_score": 0.0,
                "prediction_class": 0
            }
        
        try:
            # 1. è¯»å–RGBè§†é¢‘
            rgb_frames = []
            cap = cv2.VideoCapture(str(rgb_video_path))
            frame_count = 0
            while frame_count < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                rgb_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                frame_count += 1
            cap.release()
            
            # 2. è¯»å–æ·±åº¦è§†é¢‘
            depth_frames = []
            cap = cv2.VideoCapture(str(depth_video_path))
            frame_count = 0
            while frame_count < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                # è½¬æ¢ä¸ºç°åº¦å›¾
                if len(frame.shape) == 3:
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                depth_frames.append(frame)
                frame_count += 1
            cap.release()
            
            # 3. è¯»å–çœ¼åŠ¨æ•°æ®ï¼ˆæ”¯æŒJSONLæ ¼å¼ï¼‰
            eyetrack_samples = []
            if eyetrack_json_path and Path(eyetrack_json_path).exists():
                with open(eyetrack_json_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            data = json.loads(line)
                            # æå–8ç»´çœ¼åŠ¨ç‰¹å¾ï¼šgaze(2) + eye_position(6)
                            # gaze_point: [x, y]
                            # eye_position: [left_x, left_y, left_z, right_x, right_y, right_z]
                            gaze = data.get('gaze_point', [0.0, 0.0])
                            eye_pos = data.get('eye_position', [0.0] * 6)
                            
                            # ç¡®ä¿ç»´åº¦æ­£ç¡®
                            if len(gaze) < 2:
                                gaze = [0.0, 0.0]
                            if len(eye_pos) < 6:
                                eye_pos = [0.0] * 6
                            
                            # ç»„åˆä¸º8ç»´ç‰¹å¾
                            sample = list(gaze[:2]) + list(eye_pos[:6])
                            eyetrack_samples.append(sample)
            
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"ï¿½ ç–²åŠ³åº¦åˆ†æ - æ–‡ä»¶æ¨¡å¼")
            self.logger.info(f"{'='*60}")
            self.logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
            self.logger.info(f"   RGBå¸§æ•°: {len(rgb_frames)}")
            self.logger.info(f"   æ·±åº¦å¸§æ•°: {len(depth_frames)}")
            self.logger.info(f"   çœ¼åŠ¨æ ·æœ¬æ•°: {len(eyetrack_samples)}")
            self.logger.info(f"ğŸ“‚ æ–‡ä»¶è·¯å¾„:")
            self.logger.info(f"   RGBè§†é¢‘: {Path(rgb_video_path).name}")
            self.logger.info(f"   æ·±åº¦è§†é¢‘: {Path(depth_video_path).name}")
            if eyetrack_json_path:
                self.logger.info(f"   çœ¼åŠ¨æ•°æ®: {Path(eyetrack_json_path).name}")
            
            # 4. æå–ç‰¹å¾
            self.logger.info(f"ğŸ” æå–é¢éƒ¨å’Œçœ¼åŠ¨ç‰¹å¾...")
            frames = min(len(rgb_frames), len(depth_frames))
            face_feat = extract_face_features_from_frames(
                rgb_frames, depth_frames, frames=frames
            ).to(self.device)
            
            eye_feat = extract_eye_features_from_samples(eyetrack_samples).to(self.device)
            
            # 5. æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output = self.model(eye_feat, face_feat)
                probs = output.cpu().numpy()[0]
                num_classes = output.shape[1]
                
                scores = np.linspace(0, 100, num_classes)
                score = float(np.dot(probs, scores))
                pred = int(np.argmax(probs))
            
            inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è¾“å‡ºæ¨ç†ç»“æœ
            fatigue_level = "æ­£å¸¸ğŸ˜Š" if score < 30 else "è½»åº¦ç–²åŠ³ğŸ˜" if score < 60 else "é‡åº¦ç–²åŠ³ğŸ˜´"
            self.logger.info(f"âœ… æ¨ç†å®Œæˆ:")
            self.logger.info(f"   ç–²åŠ³åº¦åˆ†æ•°: {round(score, 2)}")
            self.logger.info(f"   ç–²åŠ³ç­‰çº§: {fatigue_level}")
            self.logger.info(f"   é¢„æµ‹ç±»åˆ«: {pred}")
            self.logger.info(f"   æ¨ç†è€—æ—¶: {round(inference_time, 1)}ms")
            self.logger.info(f"   æ¨ç†æ¨¡å¼: æ–‡ä»¶æ¨¡å¼")
            self.logger.info(f"{'='*60}\n")
            
            return {
                "status": "success",
                "fatigue_score": round(score, 2),
                "prediction_class": pred,
                "num_rgb_frames": len(rgb_frames),
                "num_depth_frames": len(depth_frames),
                "num_eyetrack_samples": len(eyetrack_samples),
                "inference_mode": "file",
                "inference_time_ms": round(inference_time, 1)
            }
            
        except Exception as e:
            self.logger.error(f"ä»æ–‡ä»¶æ¨ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "fatigue_score": 0.0,
                "prediction_class": 0
            }
    
    def _infer_from_base64(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ä»base64æ•°æ®æ¨ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        import time
        start_time = time.time()
        
        rgb_b64_list = data.get("rgb_frames", [])
        depth_b64_list = data.get("depth_frames", [])
        eyetrack_samples = data.get("eyetrack_samples", [])
        elapsed = data.get("elapsed_time", 0.0)
        
        if not rgb_b64_list or not depth_b64_list:
            return {
                "status": "error",
                "error": "ç¼ºå°‘å¿…éœ€çš„å›¾åƒæ•°æ®",
                "fatigue_score": 0.0,
                "prediction_class": 0
            }
        
        try:
            # 1. è§£ç å›¾åƒ
            rgb_frames = []
            depth_frames = []
            
            for rgb_b64 in rgb_b64_list:
                rgb_bytes = base64.b64decode(rgb_b64)
                rgb_image = Image.open(io.BytesIO(rgb_bytes)).convert("RGB")
                rgb_array = np.array(rgb_image)
                rgb_frames.append(rgb_array)
            
            for depth_b64 in depth_b64_list:
                depth_bytes = base64.b64decode(depth_b64)
                depth_image = Image.open(io.BytesIO(depth_bytes)).convert("L")
                depth_array = np.array(depth_image)
                depth_frames.append(depth_array)
            
            # 2. æå–ç‰¹å¾
            frames = min(len(rgb_frames), len(depth_frames))
            face_feat = extract_face_features_from_frames(
                rgb_frames, depth_frames, frames=frames
            ).to(self.device)
            
            eye_feat = extract_eye_features_from_samples(eyetrack_samples).to(self.device)
            
            # 3. æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output = self.model(eye_feat, face_feat)
                probs = output.cpu().numpy()[0]
                num_classes = output.shape[1]
                
                # åŠ æƒè®¡ç®—åˆ†æ•°
                scores = np.linspace(0, 100, num_classes)
                score = float(np.dot(probs, scores))
                pred = int(np.argmax(probs))
            
            inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            self.logger.info(
                f"âœ… Base64æ¨ç†å®Œæˆ: åˆ†æ•°={round(score, 2)}, ç±»åˆ«={pred}, è€—æ—¶={round(inference_time, 1)}ms"
            )
            
            return {
                "status": "success",
                "fatigue_score": round(score, 2),
                "prediction_class": pred,
                "elapsed_time": round(elapsed, 2),
                "num_rgb_frames": len(rgb_frames),
                "num_depth_frames": len(depth_frames),
                "num_eyetrack_samples": len(eyetrack_samples),
                "inference_mode": "base64",
                "inference_time_ms": round(inference_time, 1)
            }
            
        except Exception as e:
            self.logger.error(f"ç–²åŠ³åº¦æ¨ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "fatigue_score": 0.0,
                "prediction_class": 0
            }
    
    def cleanup(self) -> None:
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if hasattr(self, 'model'):
            del self.model
        
        if HAS_TORCH and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        import gc
        gc.collect()


__all__ = ["FatigueModel"]

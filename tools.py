"""

- build_store_dirï¼šæž„å»º"root/username/æ—¶é—´æˆ³"å­˜å‚¨ç›®å½•
- transcribe_audioï¼šä½¿ç”¨ Faster-Whisper å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡å­—
"""

import datetime
import os
import threading
import time
from typing import Optional, List, Dict
from faster_whisper import WhisperModel
from opencc import OpenCC


def build_store_dir(root: str, username: str, start_time: Optional[str] = None) -> str:
    """æž„å»ºå­˜å‚¨ç›®å½•ï¼Œè·¯å¾„æ ¼å¼ï¼šroot/username/æ—¶é—´æˆ³ã€‚"""
    ts = start_time or datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    store_dir = os.path.join(root, username, ts)
    os.makedirs(store_dir, exist_ok=True)
    return store_dir


def transcribe_audio(audio_path: str, language: str = "zh", local_files_only = False) -> str:
    """ä½¿ç”¨ Faster-Whisper å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡å­—ï¼ˆå›ºå®š CPU + int8ï¼ŒåŸºç¡€ä¼˜åŒ–ï¼‰"""
    model = WhisperModel(
        "base",
        device="cpu",
        compute_type="int8",
        local_files_only=local_files_only
    )

    # è½¬å†™éŸ³é¢‘ï¼šå¯ç”¨ VADã€æŸæœç´¢ï¼Œå›ºå®šä¸­æ–‡ï¼Œå‡å°‘é‡å¤ä¸Žå¹»è§‰
    segments, info = model.transcribe(
        audio_path,
        language=language,
        vad_filter=True,
        beam_size=5,
        best_of=5,
        temperature=0.0,
        condition_on_previous_text=False,
        word_timestamps=False,
    )
    # åˆå¹¶æ–‡æœ¬å¹¶åŽ»é™¤ç›¸é‚»é‡å¤
    merged = []
    prev = None
    for seg in segments:
        t = (seg.text or "").strip()
        if not t:
            continue
        if t != prev:
            merged.append(t)
            prev = t
    text = "".join(merged)
    # ç¹è½¬ç®€ï¼ˆt2sï¼‰
    cc = OpenCC('t2s')
    return cc.convert(text)


class AsyncSpeechRecognizer:
    """å¼‚æ­¥è¯­éŸ³è¯†åˆ«ç®¡ç†å™¨ï¼Œé¿å…é˜»å¡žä¸»çº¿ç¨‹ã€‚"""

    def __init__(self) -> None:
        self._queue: List[Dict] = []
        self._results: List[Dict] = []
        self._lock = threading.Lock()
        self._running = False
        self._worker: Optional[threading.Thread] = None

    def add(self, *, audio_path: str, question_index: int, question_text: str) -> None:
        with self._lock:
            self._queue.append({
                "audio_path": audio_path,
                "question_index": question_index,
                "question_text": question_text,
                "timestamp": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            })
        if not self._running:
            self._start()

    def _start(self) -> None:
        if self._worker and self._worker.is_alive():
            return
        self._running = True
        self._worker = threading.Thread(target=self._loop, daemon=True)
        self._worker.start()

    def _loop(self) -> None:
        while self._running:
            task: Optional[Dict] = None
            with self._lock:
                if self._queue:
                    task = self._queue.pop(0)
            if task is None:
                time.sleep(0.05)
                continue
            try:
                print(f"ðŸŽ¤ å¼€å§‹è¯†åˆ«ç¬¬ {task['question_index']} é¢˜éŸ³é¢‘...")
                text = transcribe_audio(task["audio_path"], language="zh", local_files_only=False)
                if text and text.strip():
                    with self._lock:
                        result = {
                            "question_index": task["question_index"],
                            "question_text": task["question_text"],
                            "recognized_text": text.strip(),
                            "audio_path": task["audio_path"],
                            "timestamp": task["timestamp"],
                        }
                        self._results.append(result)
                    
                    # è¾“å‡ºè¯†åˆ«æˆåŠŸæç¤º
                    print(f"âœ… è¯­éŸ³è¯†åˆ«æˆåŠŸï¼")
                    print(f"   é¢˜ç›®: {task['question_text']}")
                    print(f"   è¯†åˆ«ç»“æžœ: {text.strip()}")
                    print(f"   éŸ³é¢‘æ–‡ä»¶: {task['audio_path']}")
                    print(f"   æ—¶é—´: {task['timestamp']}")
                    print("-" * 50)
                else:
                    with self._lock:
                        result = {
                            "question_index": task["question_index"],
                            "question_text": task["question_text"],
                            "recognized_text": 'è¯†åˆ«ç»“æžœä¸ºç©º',
                            "audio_path": task["audio_path"],
                            "timestamp": task["timestamp"],
                        }                    
                    print(f"âš ï¸  ç¬¬ {task['question_index'] + 1} é¢˜è¯†åˆ«ç»“æžœä¸ºç©º")
                    print(f"   éŸ³é¢‘æ–‡ä»¶: {task['audio_path']}")
                    print("-" * 50)
                if task["question_index"]>5:
                    print('5æ¡è¯­éŸ³å‡å·²è¯†åˆ«å®Œæˆ')
            except Exception as e:
                print(f"âŒ ç¬¬ {task['question_index'] + 1} é¢˜è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
                print(f"   éŸ³é¢‘æ–‡ä»¶: {task['audio_path']}")
                print("-" * 50)

    def get_results(self) -> List[Dict]:
        with self._lock:
            return list(self._results)

    def clear(self) -> None:
        with self._lock:
            self._queue.clear()
            self._results.clear()


_GLOBAL_RECOGNIZER: Optional[AsyncSpeechRecognizer] = None


def _get_recognizer() -> AsyncSpeechRecognizer:
    global _GLOBAL_RECOGNIZER
    if _GLOBAL_RECOGNIZER is None:
        _GLOBAL_RECOGNIZER = AsyncSpeechRecognizer()
    return _GLOBAL_RECOGNIZER


def add_audio_for_recognition(audio_path: str, question_index: int, question_text: str) -> None:
    _get_recognizer().add(audio_path=audio_path, question_index=question_index, question_text=question_text)


def get_recognition_results() -> List[Dict]:
    return _get_recognizer().get_results()


def clear_recognition_results() -> None:
    _get_recognizer().clear()
# Model Backends - ç‹¬ç«‹æ¨¡å‹åç«¯æœåŠ¡

æœ¬ç›®å½•åŒ…å«æ‰€æœ‰ç‹¬ç«‹è¿è¡Œçš„AIæ¨¡å‹åç«¯æœåŠ¡ã€‚æ¯ä¸ªåç«¯è¿è¡Œåœ¨ç‹¬ç«‹çš„Pythonç¯å¢ƒä¸­,é€šè¿‡WebSocketä¸ä¸»åç«¯é€šä¿¡ã€‚

---

## ğŸ“ ç›®å½•ç»“æ„

```
model_backends/
â”œâ”€â”€ base/                      # æŠ½è±¡åŸºç±»
â”‚   â””â”€â”€ base_backend.py        # BaseModelBackend - æ‰€æœ‰åç«¯çš„çˆ¶ç±»
â”œâ”€â”€ fatigue_backend/           # ç–²åŠ³åº¦æ¨¡å‹åç«¯ âœ…
â”‚   â”œâ”€â”€ main.py               # å®Œæ•´å®ç°
â”‚   â”œâ”€â”€ requirements.txt      # ç‹¬ç«‹ä¾èµ–
â”‚   â””â”€â”€ README.md             # ä½¿ç”¨è¯´æ˜
â”œâ”€â”€ emotion_backend/           # æƒ…ç»ªæ¨¡å‹åç«¯ âœ…
â”‚   â”œâ”€â”€ main.py               # å®Œæ•´å®ç°
â”‚   â”œâ”€â”€ requirements.txt      # ç‹¬ç«‹ä¾èµ–
â”‚   â””â”€â”€ README.md             # ä½¿ç”¨è¯´æ˜
â””â”€â”€ eeg_backend/               # è„‘ç”µæ¨¡å‹åç«¯ (å¾…å®ç°) âš ï¸
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»ºæ–°çš„æ¨¡å‹åç«¯

ä½¿ç”¨ `fatigue_backend` æˆ– `emotion_backend` ä½œä¸ºæ¨¡æ¿:

```bash
# å¤åˆ¶æ¨¡æ¿
cp -r fatigue_backend your_backend

# ä¿®æ”¹å®ç°
cd your_backend
# ç¼–è¾‘ main.py, requirements.txt, README.md
```

### 2. å®ç°å¿…éœ€çš„æ–¹æ³•

ç»§æ‰¿ `BaseModelBackend` å¹¶å®ç°ä¸‰ä¸ªæŠ½è±¡æ–¹æ³•:

```python
from model_backends.base.base_backend import BaseModelBackend

class YourBackend(BaseModelBackend):
    def initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹ - åœ¨å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡"""
        self.model = load_your_model()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()
    
    def process_inference(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æ¨ç†è¯·æ±‚ - æ¯æ¬¡è¯·æ±‚éƒ½ä¼šè°ƒç”¨"""
        # 1. è§£ç è¾“å…¥æ•°æ®
        image_base64 = data.get("image")
        image = decode_base64_image(image_base64)
        
        # 2. é¢„å¤„ç†
        tensor = self.preprocess(image)
        
        # 3. æ¨ç†
        with torch.no_grad():
            output = self.model(tensor)
        
        # 4. åå¤„ç†
        result = self.postprocess(output)
        
        return result
    
    def cleanup(self):
        """æ¸…ç†èµ„æº - åœ¨åœæ­¢æ—¶è°ƒç”¨"""
        if hasattr(self, 'model'):
            del self.model
        torch.cuda.empty_cache()
```

### 3. é…ç½®ç«¯å£

åœ¨ `config/models.yaml` ä¸­æ·»åŠ é…ç½®:

```yaml
model_backends:
  your_backend:
    host: "localhost"
    port: 8769  # é€‰æ‹©æœªä½¿ç”¨çš„ç«¯å£
    enabled: false  # å¼€å‘æ—¶è®¾ä¸ºfalse,éƒ¨ç½²æ—¶è®¾ä¸ºtrue
    reconnect_interval: 5
    max_reconnect_attempts: 10
    request_timeout: 30
```

### 4. å¯åŠ¨åç«¯

```bash
cd your_backend

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv .venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Windows)
.venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¯åŠ¨åç«¯
python main.py --host localhost --port 8769
```

---

## ğŸ”Œ WebSocketåè®®

æ‰€æœ‰æ¨¡å‹åç«¯å¿…é¡»éµå¾ªç»Ÿä¸€çš„WebSocketåè®®ã€‚

### æ¶ˆæ¯æ ¼å¼

#### æ¨ç†è¯·æ±‚ (ä¸»åç«¯ â†’ æ¨¡å‹åç«¯)

```json
{
  "type": "inference_request",
  "request_id": "unique-uuid",
  "data": {
    "image": "base64-encoded-image",
    "timestamp": 1234567890.123,
    // ... å…¶ä»–æ¨¡å‹ç‰¹å®šçš„æ•°æ®
  }
}
```

#### æ¨ç†å“åº” (æ¨¡å‹åç«¯ â†’ ä¸»åç«¯)

```json
{
  "type": "inference_response",
  "request_id": "same-uuid-from-request",
  "data": {
    "prediction": 0.85,
    "confidence": 0.92,
    // ... æ¨¡å‹è¾“å‡ºç»“æœ
  }
}
```

#### é”™è¯¯å“åº”

```json
{
  "type": "error",
  "request_id": "same-uuid-from-request",
  "error": "Detailed error message"
}
```

#### å¥åº·æ£€æŸ¥

è¯·æ±‚:
```json
{
  "type": "health_check"
}
```

å“åº”:
```json
{
  "type": "pong",
  "status": "healthy"
}
```

---

## ğŸ“¦ ä¾èµ–ç®¡ç†

æ¯ä¸ªåç«¯éƒ½æœ‰ç‹¬ç«‹çš„ `requirements.txt`:

### å¿…éœ€ä¾èµ–

```txt
# WebSocketé€šä¿¡
websockets>=10.0

# å¦‚æœéœ€è¦è®¿é—®åŸºç±»
# æ³¨æ„: åŸºç±»æ–‡ä»¶åº”è¯¥åœ¨Pythonè·¯å¾„ä¸­
```

### æ¡†æ¶ç‰¹å®šä¾èµ–

**PyTorchåç«¯**:
```txt
torch>=1.9.0
torchvision>=0.10.0
```

**TensorFlowåç«¯**:
```txt
tensorflow>=2.6.0
# æˆ–è€…
tensorflow-gpu>=2.6.0
```

**è„‘ç”µå¤„ç†åç«¯**:
```txt
mne>=0.24.0
numpy>=1.19.0
scipy>=1.5.0
```

### é€šç”¨å·¥å…·ä¾èµ–

```txt
pillow>=8.0.0  # å›¾åƒå¤„ç†
numpy>=1.19.0  # æ•°ç»„æ“ä½œ
opencv-python>=4.5.0  # è®¡ç®—æœºè§†è§‰
```

---

## ğŸ—‚ï¸ é¡¹ç›®æ¨¡æ¿

æ¯ä¸ªæ¨¡å‹åç«¯åº”è¯¥åŒ…å«ä»¥ä¸‹æ–‡ä»¶:

```
your_backend/
â”œâ”€â”€ main.py                 # ä¸»å…¥å£æ–‡ä»¶
â”œâ”€â”€ requirements.txt        # Pythonä¾èµ–
â”œâ”€â”€ README.md               # ä½¿ç”¨è¯´æ˜
â”œâ”€â”€ model/                  # æ¨¡å‹æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ weights.pth        # æ¨¡å‹æƒé‡
â”‚   â””â”€â”€ config.yaml        # æ¨¡å‹é…ç½®
â””â”€â”€ utils/                  # å·¥å…·å‡½æ•°
    â”œâ”€â”€ preprocess.py      # é¢„å¤„ç†
    â””â”€â”€ postprocess.py     # åå¤„ç†
```

### main.py æ¨¡æ¿

```python
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from model_backends.base.base_backend import BaseModelBackend

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class YourBackend(BaseModelBackend):
    """æ‚¨çš„æ¨¡å‹åç«¯å®ç°"""
    
    def initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        logger.info("Initializing model...")
        # TODO: åŠ è½½æ¨¡å‹
        logger.info("Model initialized successfully")
    
    def process_inference(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æ¨ç†è¯·æ±‚"""
        logger.info("Processing inference request")
        # TODO: å®ç°æ¨ç†é€»è¾‘
        result = {"prediction": 0.0}
        return result
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("Cleaning up resources")
        # TODO: æ¸…ç†æ¨¡å‹

def main():
    parser = argparse.ArgumentParser(description='Your Backend Server')
    parser.add_argument('--host', type=str, default='localhost', help='Host to bind')
    parser.add_argument('--port', type=int, default=8769, help='Port to bind')
    args = parser.parse_args()
    
    backend = YourBackend(host=args.host, port=args.port)
    
    try:
        logger.info(f"Starting YourBackend on ws://{args.host}:{args.port}")
        backend.start()
    except KeyboardInterrupt:
        logger.info("Shutting down...")
        backend.stop()

if __name__ == "__main__":
    main()
```

---

## ğŸ§ª æµ‹è¯•

### å•ç‹¬æµ‹è¯•åç«¯

ä½¿ç”¨Pythonè„šæœ¬æµ‹è¯•WebSocketè¿æ¥:

```python
import asyncio
import websockets
import json

async def test_backend():
    uri = "ws://localhost:8769"
    
    async with websockets.connect(uri) as websocket:
        # 1. å¥åº·æ£€æŸ¥
        await websocket.send(json.dumps({
            "type": "health_check"
        }))
        response = await websocket.recv()
        print(f"Health check: {response}")
        
        # 2. æ¨ç†è¯·æ±‚
        await websocket.send(json.dumps({
            "type": "inference_request",
            "request_id": "test-123",
            "data": {
                "image": "base64-encoded-image"
            }
        }))
        response = await websocket.recv()
        print(f"Inference result: {response}")

asyncio.run(test_backend())
```

### é›†æˆæµ‹è¯•

è¿è¡Œæ¶æ„æµ‹è¯•:

```bash
cd project-root
python tests/test_model_backend_architecture.py
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹ä¼˜åŒ–

- **é‡åŒ–**: INT8é‡åŒ–å¯å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶é—´
- **å‰ªæ**: ç§»é™¤ä¸é‡è¦çš„æƒé‡
- **è’¸é¦**: è®­ç»ƒæ›´å°çš„å­¦ç”Ÿæ¨¡å‹

### 2. æ‰¹å¤„ç†

å¦‚æœæ”¯æŒ,å¯ä»¥æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚:

```python
def process_inference(self, data: Dict[str, Any]) -> Dict[str, Any]:
    images = data.get("images", [data.get("image")])  # æ”¯æŒå•ä¸ªæˆ–æ‰¹é‡
    batch = self.preprocess_batch(images)
    results = self.model(batch)
    return self.postprocess_batch(results)
```

### 3. GPUä¼˜åŒ–

```python
def initialize_model(self):
    if torch.cuda.is_available():
        self.device = torch.device("cuda")
        # å¯ç”¨cudnnè‡ªåŠ¨ä¼˜åŒ–
        torch.backends.cudnn.benchmark = True
    else:
        self.device = torch.device("cpu")
    
    self.model.to(self.device)
    self.model.eval()
    
    # é¢„çƒ­GPU
    dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
    with torch.no_grad():
        _ = self.model(dummy_input)
```

### 4. ç¼“å­˜

å¯¹äºé‡å¤çš„é¢„å¤„ç†ç»“æœå¯ä»¥ç¼“å­˜:

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def preprocess(self, image_hash):
    # é¢„å¤„ç†é€»è¾‘
    pass
```

---

## ğŸ› è°ƒè¯•æŠ€å·§

### 1. è¯¦ç»†æ—¥å¿—

```python
import logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def process_inference(self, data: Dict[str, Any]) -> Dict[str, Any]:
    logger.debug(f"Received data keys: {data.keys()}")
    logger.debug(f"Image size: {len(data.get('image', ''))}")
    # ... æ¨ç†é€»è¾‘
    logger.debug(f"Result: {result}")
    return result
```

### 2. æ€§èƒ½åˆ†æ

```python
import time

def process_inference(self, data: Dict[str, Any]) -> Dict[str, Any]:
    start = time.time()
    
    # é¢„å¤„ç†
    t1 = time.time()
    image = self.preprocess(data)
    logger.info(f"Preprocess: {(time.time() - t1)*1000:.2f}ms")
    
    # æ¨ç†
    t2 = time.time()
    output = self.model(image)
    logger.info(f"Inference: {(time.time() - t2)*1000:.2f}ms")
    
    # åå¤„ç†
    t3 = time.time()
    result = self.postprocess(output)
    logger.info(f"Postprocess: {(time.time() - t3)*1000:.2f}ms")
    
    logger.info(f"Total: {(time.time() - start)*1000:.2f}ms")
    return result
```

### 3. å†…å­˜ç›‘æ§

```python
import torch

def cleanup(self):
    if hasattr(self, 'model'):
        del self.model
    
    if torch.cuda.is_available():
        logger.info(f"GPU memory before cleanup: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
        torch.cuda.empty_cache()
        logger.info(f"GPU memory after cleanup: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **æ¶æ„æ–‡æ¡£**: `../MULTI_MODEL_BACKEND_ARCHITECTURE.md`
- **å¿«é€Ÿæ¥å…¥**: `../MULTI_MODEL_BACKEND_QUICKSTART.md`
- **æ¥å£è¯´æ˜**: `../MULTI_MODEL_BACKEND_INTERFACES.md`
- **å®Œæ•´æ€§æ£€æŸ¥**: `../MULTI_MODEL_BACKEND_CHECKLIST.md`
- **å¿«é€Ÿå‚è€ƒ**: `../MULTI_MODEL_BACKEND_QUICK_REF.md`

---

## ğŸ”— ç«¯å£åˆ†é…

| åç«¯ | ç«¯å£ | çŠ¶æ€ | æ¡†æ¶ |
|-----|------|------|------|
| ä¸»åç«¯ | 8765 | âœ… è¿è¡Œä¸­ | - |
| fatigue | 8767 | âœ… å·²å®ç° | PyTorch |
| emotion | 8768 | âœ… å·²å®ç° | PyTorch + Transformers |
| eeg | 8769 | âš ï¸ å¾…å®ç° | MNE-Python |
| (é¢„ç•™) | 8770+ | - | - |

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ç‹¬ç«‹è™šæ‹Ÿç¯å¢ƒ**: æ¯ä¸ªåç«¯ä½¿ç”¨ç‹¬ç«‹çš„venv
2. **æ˜ç¡®çš„æ—¥å¿—**: ä½¿ç”¨loggerè€Œä¸æ˜¯print
3. **ä¼˜é›…å…³é—­**: å¤„ç†KeyboardInterrupt,æ¸…ç†èµ„æº
4. **é”™è¯¯å¤„ç†**: æ•è·å¼‚å¸¸,è¿”å›erroræ¶ˆæ¯
5. **é…ç½®ä¼˜äºç¡¬ç¼–ç **: ä½¿ç”¨é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°
6. **ç‰ˆæœ¬æ§åˆ¶**: åœ¨requirements.txtä¸­å›ºå®šä¾èµ–ç‰ˆæœ¬
7. **æ–‡æ¡£å®Œå–„**: ä¿æŒREADME.mdæ›´æ–°

---

## â“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©ç«¯å£?
A: ä½¿ç”¨8766-8999èŒƒå›´,é¿å…å¸¸ç”¨ç«¯å£å†²çªã€‚

### Q: å¦‚ä½•å¤„ç†å¤§å›¾åƒ?
A: åœ¨å‘é€å‰å‹ç¼©,æˆ–ä½¿ç”¨æ›´é«˜æ•ˆçš„ç¼–ç  (å¦‚JPEGè€ŒéPNG)ã€‚

### Q: å¦‚ä½•æ”¯æŒå¤šGPU?
A: ä½¿ç”¨DataParallelæˆ–DistributedDataParallel:
```python
self.model = torch.nn.DataParallel(self.model)
```

### Q: å¦‚ä½•ç›‘æ§åç«¯çŠ¶æ€?
A: ä¸»åç«¯ä¼šå®šæœŸå‘é€health_checkè¯·æ±‚ã€‚

### Q: å¦‚ä½•å¤„ç†æ¨¡å‹æ›´æ–°?
A: åœæ­¢åç«¯,æ›´æ–°æ¨¡å‹æ–‡ä»¶,é‡å¯åç«¯ã€‚è€ƒè™‘ä½¿ç”¨çƒ­é‡è½½æœºåˆ¶ã€‚

---

**æœ€åæ›´æ–°**: 2025-01-XX
**ç»´æŠ¤è€…**: GitHub Copilot

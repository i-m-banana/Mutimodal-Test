"""æƒ…ç»ªæ¨¡å‹åç«¯

ä½¿ç”¨emotion_fatigue_inferä¸­çš„æƒ…ç»ªæ¨ç†æ¨¡å‹
å¤„ç†è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬æ•°æ®,è¾“å‡ºæƒ…ç»ªåˆ†æ•°
"""

import base64
import io
import logging
import sys
import asyncio
from pathlib import Path
from typing import Any, Dict, List
import time
import tempfile
import os

# æ·»åŠ baseè·¯å¾„å’Œemotion_fatigue_inferè·¯å¾„
base_path = Path(__file__).parent.parent / "base"
emotion_fatigue_path = Path(__file__).parent.parent / "emotion_fatigue_infer"
emotion_path = emotion_fatigue_path / "emotion"

sys.path.insert(0, str(base_path))
sys.path.insert(0, str(emotion_fatigue_path))
sys.path.insert(0, str(emotion_path))

from base_backend import BaseModelBackend

try:
    import torch
    import numpy as np
    import cv2
    import soundfile as sf
    from transformers import VivitImageProcessor, Wav2Vec2Processor, AutoTokenizer, AutoModel, Wav2Vec2Model
    from emotion.inference_standalone_all import (
        SimpleMultimodalClassifier,
        extract_vision_feature,
        extract_audio_feature,
        extract_text_feature
    )
except ImportError as e:
    print("é”™è¯¯: ç¼ºå°‘ä¾èµ–åº“")
    print(f"è¯¦ç»†é”™è¯¯: {e}")
    print("è¯·å®‰è£…: pip install torch numpy opencv-python soundfile transformers")
    sys.exit(1)


class EmotionBackend(BaseModelBackend):
    """æƒ…ç»ªæ¨¡å‹åç«¯å®ç°
    
    åŠŸèƒ½:
    - å¤„ç†è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬æ•°æ®
    - æå–è§†è§‰ã€éŸ³é¢‘ã€æ–‡æœ¬ç‰¹å¾
    - è¾“å‡ºæƒ…ç»ªåˆ†æ•° (0-100)
    """
    
    async def initialize_model(self) -> None:
        """åŠ è½½æƒ…ç»ªæ¨¡å‹"""
        self.logger.info("æ­£åœ¨åŠ è½½æƒ…ç»ªæ¨¡å‹...")
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„ - ä»æ ¹ç›®å½•çš„models_dataæ–‡ä»¶å¤¹åŠ è½½
        project_root = Path(__file__).parent.parent.parent
        models_dir = project_root / "models_data" / "emotion_models"
        self.model_path = models_dir / "best_model.pt"
        # é¢„è®­ç»ƒæ¨¡å‹ä¹Ÿç§»åˆ°models_data
        pretrained_models_dir = project_root / "models_data" / "emotion_pretrained_models"
        self.model_dir = pretrained_models_dir
        
        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
        
        # è®¾ç½®è®¾å¤‡
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½é¢„å¤„ç†å™¨å’Œç‰¹å¾æå–æ¨¡å‹
        print("="*60)
        print("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        print("="*60)
        
        self.logger.info("åŠ è½½è§†è§‰å¤„ç†å™¨...")
        print("  [1/6] åŠ è½½è§†è§‰å¤„ç†å™¨ (TIMESFORMER)...")
        self.vision_processor = VivitImageProcessor.from_pretrained(str(self.model_dir / "TIMESFORMER"))
        print("  âœ“ è§†è§‰å¤„ç†å™¨åŠ è½½å®Œæˆ")
        
        self.logger.info("åŠ è½½éŸ³é¢‘å¤„ç†å™¨...")
        print("  [2/6] åŠ è½½éŸ³é¢‘å¤„ç†å™¨ (WAV2VEC2)...")
        self.audio_processor = Wav2Vec2Processor.from_pretrained(str(self.model_dir / "WAV2VEC2"))
        print("  âœ“ éŸ³é¢‘å¤„ç†å™¨åŠ è½½å®Œæˆ")
        
        self.logger.info("åŠ è½½æ–‡æœ¬åˆ†è¯å™¨...")
        print("  [3/6] åŠ è½½æ–‡æœ¬åˆ†è¯å™¨ (ROBBERTA)...")
        self.text_tokenizer = AutoTokenizer.from_pretrained(str(self.model_dir / "ROBBERTA"))
        print("  âœ“ æ–‡æœ¬åˆ†è¯å™¨åŠ è½½å®Œæˆ")
        
        self.logger.info("åŠ è½½è§†è§‰æ¨¡å‹...")
        print("  [4/6] åŠ è½½è§†è§‰æ¨¡å‹ (TIMESFORMER)...")
        self.vision_model = AutoModel.from_pretrained(
            str(self.model_dir / "TIMESFORMER"), 
            ignore_mismatched_sizes=True
        ).to(self.device)
        self.vision_model.eval()
        print("  âœ“ è§†è§‰æ¨¡å‹åŠ è½½å®Œæˆ")
        
        self.logger.info("åŠ è½½éŸ³é¢‘æ¨¡å‹...")
        print("  [5/6] åŠ è½½éŸ³é¢‘æ¨¡å‹ (WAV2VEC2)...")
        self.audio_model = Wav2Vec2Model.from_pretrained(
            str(self.model_dir / "WAV2VEC2"), 
            ignore_mismatched_sizes=True
        ).to(self.device)
        self.audio_model.eval()
        print("  âœ“ éŸ³é¢‘æ¨¡å‹åŠ è½½å®Œæˆ")
        
        self.logger.info("åŠ è½½æ–‡æœ¬æ¨¡å‹...")
        print("  [6/6] åŠ è½½æ–‡æœ¬æ¨¡å‹ (ROBBERTA)...")
        self.text_model = AutoModel.from_pretrained(
            str(self.model_dir / "ROBBERTA"), 
            ignore_mismatched_sizes=True
        ).to(self.device)
        self.text_model.eval()
        print("  âœ“ æ–‡æœ¬æ¨¡å‹åŠ è½½å®Œæˆ")
        print("="*60)
        
        # é¢„çƒ­:æå–ä¸€æ¬¡ç‰¹å¾ä»¥ç¡®å®šç»´åº¦
        self.logger.info("é¢„çƒ­æ¨¡å‹...")
        print("\nğŸ”¥ é¢„çƒ­æ¨¡å‹(æå–ç‰¹å¾ç»´åº¦)...")
        dummy_video = np.zeros((224, 224, 3), dtype=np.uint8)
        dummy_audio = np.zeros(16000, dtype=np.float32)
        dummy_text = "test"
        
        # åˆ›å»ºä¸´æ—¶è§†é¢‘æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.avi', delete=False) as tmp_video:
            tmp_video_path = tmp_video.name
            out = cv2.VideoWriter(tmp_video_path, cv2.VideoWriter_fourcc(*'XVID'), 30, (224, 224))
            for _ in range(8):
                out.write(dummy_video)
            out.release()
        
        # åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_audio:
            tmp_audio_path = tmp_audio.name
            sf.write(tmp_audio_path, dummy_audio, 16000)
        
        try:
            vision_feat = extract_vision_feature(tmp_video_path, self.vision_processor, self.vision_model, self.device)
            audio_feat = extract_audio_feature(tmp_audio_path, self.audio_processor, self.audio_model, self.device)
            text_feat = extract_text_feature(dummy_text, self.text_tokenizer, self.text_model, self.device)
            
            print(f"  ç‰¹å¾ç»´åº¦: è§†è§‰={vision_feat.shape}, éŸ³é¢‘={audio_feat.shape}, æ–‡æœ¬={text_feat.shape}")
            
            # åˆå§‹åŒ–åˆ†ç±»å™¨
            print(f"  åˆå§‹åŒ–åˆ†ç±»å™¨: hidden_dim=512, num_classes=2")
            self.model = SimpleMultimodalClassifier(
                vision_feat_dim=vision_feat.shape[-1],
                audio_feat_dim=audio_feat.shape[-1],
                text_feat_dim=text_feat.shape[-1],
                hidden_dim=512,
                num_classes=2,
            ).to(self.device)
            
            # åŠ è½½æƒé‡
            print(f"  åŠ è½½æ¨¡å‹æƒé‡: {self.model_path.name}")
            self.model.load_state_dict(torch.load(str(self.model_path), map_location=self.device))
            self.model.eval()
            print(f"  âœ“ é¢„çƒ­å®Œæˆ")
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_video_path):
                os.remove(tmp_video_path)
            if os.path.exists(tmp_audio_path):
                os.remove(tmp_audio_path)
        
        print("="*60)
        self.logger.info("âœ… æƒ…ç»ªæ¨¡å‹åŠ è½½å®Œæˆ")
    
    async def process_inference(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæƒ…ç»ªæ¨ç†
        
        Args:
            data: è¾“å…¥æ•°æ®,åŒ…å«:
                - samples: æ ·æœ¬åˆ—è¡¨,æ¯ä¸ªæ ·æœ¬åŒ…å«:
                    - video_b64: è§†é¢‘æ–‡ä»¶base64ç¼–ç 
                    - audio_b64: éŸ³é¢‘æ–‡ä»¶base64ç¼–ç 
                    - text: è¯†åˆ«çš„æ–‡æœ¬
                    - question_index: é—®é¢˜ç¼–å· (å¯é€‰)
        
        Returns:
            æ¨ç†ç»“æœ:
                - emotion_score: æƒ…ç»ªåˆ†æ•° (0-100)
                - sample_scores: æ¯ä¸ªæ ·æœ¬çš„åˆ†æ•°
                - predictions: æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«
        """
        start_time = time.time()
        
        samples = data.get("samples", [])
        
        # æ‰“å°è¾“å…¥ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"ğŸ“¥ æ¥æ”¶åˆ°æƒ…ç»ªæ¨ç†è¯·æ±‚")
        print(f"{'='*60}")
        print(f"  æ ·æœ¬æ•°é‡: {len(samples)}")
        
        if not samples:
            print(f"âš ï¸  æ•°æ®ä¸è¶³,è¿”å›é»˜è®¤å€¼")
            print(f"{'='*60}\n")
            return {
                "emotion_score": 0.0,
                "sample_scores": [],
                "predictions": [],
                "message": "æ•°æ®ä¸è¶³,è¿”å›é»˜è®¤å€¼"
            }
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜å‚¨è§†é¢‘å’ŒéŸ³é¢‘
        temp_files = []
        try:
            logits_list = []
            sample_results = []
            
            for idx, sample in enumerate(samples):
                video_b64 = sample.get("video_b64", "")
                audio_b64 = sample.get("audio_b64", "")
                text = sample.get("text", "")
                question_index = sample.get("question_index", idx + 1)
                
                print(f"\nğŸ¬ å¤„ç†æ ·æœ¬ {idx+1}/{len(samples)} (é—®é¢˜ {question_index})")
                print(f"  åŸå§‹æ–‡æœ¬: \"{text}\"")
                print(f"  è§†é¢‘å¤§å°: {len(video_b64)/1024:.1f}KB")
                print(f"  éŸ³é¢‘å¤§å°: {len(audio_b64)/1024:.1f}KB")
                
                # è§£ç å¹¶ä¿å­˜è§†é¢‘
                video_bytes = base64.b64decode(video_b64)
                tmp_video = tempfile.NamedTemporaryFile(suffix='.avi', delete=False)
                tmp_video.write(video_bytes)
                tmp_video.close()
                temp_files.append(tmp_video.name)
                
                # è§£ç å¹¶ä¿å­˜éŸ³é¢‘
                audio_bytes = base64.b64decode(audio_b64)
                tmp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                tmp_audio.write(audio_bytes)
                tmp_audio.close()
                temp_files.append(tmp_audio.name)
                
                # æå–ç‰¹å¾
                print(f"  ğŸ” æå–ç‰¹å¾...")
                sample_start = time.time()
                v_feat = extract_vision_feature(tmp_video.name, self.vision_processor, self.vision_model, self.device)
                print(f"    âœ“ è§†è§‰ç‰¹å¾: {v_feat.shape}")
                
                a_feat = extract_audio_feature(tmp_audio.name, self.audio_processor, self.audio_model, self.device)
                print(f"    âœ“ éŸ³é¢‘ç‰¹å¾: {a_feat.shape}")
                
                # æ˜¾ç¤ºæ–‡æœ¬é¢„å¤„ç†
                print(f"  ğŸ“ æ–‡æœ¬é¢„å¤„ç†:")
                print(f"    åŸå§‹æ–‡æœ¬: \"{text}\"")
                # ç®€å•çš„é¢„å¤„ç†(ä¸preprocess_textä¸€è‡´)
                preprocessed_text = " ".join([
                    '@user' if t.startswith('@') and len(t) > 1 else 
                    'http' if t.startswith('http') else t
                    for t in str(text).split(" ")
                ])
                print(f"    é¢„å¤„ç†å: \"{preprocessed_text}\"")
                
                t_feat = extract_text_feature(text, self.text_tokenizer, self.text_model, self.device)
                print(f"    âœ“ æ–‡æœ¬ç‰¹å¾: {t_feat.shape}")
                
                feat_time = time.time() - sample_start
                print(f"  â±ï¸  ç‰¹å¾æå–è€—æ—¶: {feat_time*1000:.0f}ms")
                
                # æ¨ç†
                print(f"  ğŸ§  æ¨¡å‹æ¨ç†ä¸­...")
                print(f"    è¾“å…¥å½¢çŠ¶: v={v_feat.shape}, a={a_feat.shape}, t={t_feat.shape}")
                
                with torch.no_grad():
                    logits = self.model(v_feat.unsqueeze(0), a_feat.unsqueeze(0), t_feat.unsqueeze(0))  # [1, 2]
                    print(f"    æ¨¡å‹logitsè¾“å‡º: {logits.cpu().numpy()}")
                    
                    probs = torch.softmax(logits, dim=1)
                    print(f"    Softmaxæ¦‚ç‡: {probs.cpu().numpy()}")
                    
                    logits_list.append(logits.squeeze(0).cpu().numpy())
                    pred = torch.argmax(logits, dim=1).item()
                    prob_values = probs.squeeze(0).cpu().numpy()
                
                sample_elapsed = time.time() - sample_start
                
                print(f"  ğŸ“Š æ ·æœ¬ç»“æœ:")
                print(f"    é¢„æµ‹ç±»åˆ«: {pred} ({'ç§¯æ' if pred == 1 else 'æ¶ˆæ'})")
                print(f"    ç±»åˆ«æ¦‚ç‡: [{prob_values[0]:.3f}, {prob_values[1]:.3f}]")
                print(f"    logitså€¼: [{logits.squeeze(0).cpu().numpy()[0]:.3f}, {logits.squeeze(0).cpu().numpy()[1]:.3f}]")
                print(f"    æ€»è€—æ—¶: {sample_elapsed*1000:.0f}ms")
                
                sample_results.append({
                    "question_index": question_index,
                    "prediction": pred,
                    "probabilities": prob_values.tolist(),
                    "inference_time_ms": round(sample_elapsed * 1000, 2)
                })
                
                self.logger.debug(f"æ ·æœ¬ {question_index} æ¨ç†å®Œæˆ: ç±»åˆ«={pred}, è€—æ—¶={sample_elapsed:.4f}ç§’")
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            print(f"\nğŸ“ˆ è®¡ç®—æœ€ç»ˆæƒ…ç»ªåˆ†æ•°...")
            logits_arr = np.array(logits_list)  # [N, 2]
            probs = torch.softmax(torch.tensor(logits_arr), dim=1).numpy()  # [N, 2]
            pos_probs = probs[:, 1]  # [N]
            
            min_prob, max_prob = pos_probs.min(), pos_probs.max()
            if max_prob - min_prob < 1e-6:
                scores = np.full_like(pos_probs, 50.0)
            else:
                scores = (pos_probs - min_prob) / (max_prob - min_prob) * 100
            
            final_score = float(np.mean(scores))
            print(f"  æ ·æœ¬åˆ†æ•°: {[f'{s:.2f}' for s in scores]}")
            print(f"  æœ€ç»ˆåˆ†æ•°: {final_score:.2f}/100")
            
        except Exception as e:
            self.logger.error(f"æ¨ç†å¤±è´¥: {e}")
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            print(f"{'='*60}\n")
            raise
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except:
                        pass
        
        inference_time = time.time() - start_time
        
        print(f"\nâœ… æƒ…ç»ªæ¨ç†å®Œæˆ")
        print(f"  æ€»è€—æ—¶: {inference_time*1000:.0f}ms")
        print(f"  å¹³å‡æ¯æ ·æœ¬: {inference_time*1000/len(samples):.0f}ms")
        print(f"{'='*60}\n")
        
        # è®°å½•æ¨ç†æ—¥å¿—
        self.logger.info(
            f"æƒ…ç»ªæ¨ç†å®Œæˆ: åˆ†æ•°={round(final_score, 2)}, "
            f"æ ·æœ¬æ•°={len(samples)}, "
            f"æ€»è€—æ—¶={inference_time*1000:.0f}ms, "
            f"å¹³å‡={inference_time*1000/len(samples):.0f}ms/æ ·æœ¬"
        )
        
        result = {
            "emotion_score": round(final_score, 2),
            "sample_scores": scores.tolist(),
            "sample_results": sample_results,
            "inference_time_ms": round(inference_time * 1000, 2),
            "num_samples": len(samples)
        }
        
        self.logger.debug(
            f"æ¨ç†å®Œæˆ: æƒ…ç»ªåˆ†æ•°={result['emotion_score']}, "
            f"æ ·æœ¬æ•°={result['num_samples']}, "
            f"æ€»è€—æ—¶={result['inference_time_ms']}ms"
        )
        
        return result
    
    async def cleanup(self) -> None:
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        self.logger.info("æ­£åœ¨æ¸…ç†æƒ…ç»ªæ¨¡å‹èµ„æº...")
        
        # é‡Šæ”¾æ¨¡å‹
        self.model = None
        self.vision_model = None
        self.audio_model = None
        self.text_model = None
        self.vision_processor = None
        self.audio_processor = None
        self.text_tokenizer = None
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        self.logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
    log_dir = Path(__file__).parent.parent.parent / "logs" / "model"
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / "emotion_backend.log"
    
    # åˆ›å»ºæ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s [%(levelname)s] %(name)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)
    
    logging.info(f"ğŸ“ æ—¥å¿—ä¿å­˜åˆ°: {log_file}")
    
    # æ¨¡å‹åç«¯é…ç½®
    config = {
        "model_type": "emotion",
        "host": "127.0.0.1",
        "port": 8768  # ä½¿ç”¨ä¸åŒç«¯å£,é¿å…ä¸å…¶ä»–åç«¯å†²çª
    }
    
    # åˆ›å»ºå¹¶è¿è¡Œåç«¯
    backend = EmotionBackend(config)
    
    print("\n" + "=" * 70)
    print("æƒ…ç»ªæ¨¡å‹åç«¯")
    print("=" * 70)
    print(f"ç›‘å¬åœ°å€: ws://{config['host']}:{config['port']}")
    print(f"æ¨¡å‹ç±»å‹: {config['model_type']}")
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 70 + "\n")
    
    backend.run()


if __name__ == "__main__":
    main()

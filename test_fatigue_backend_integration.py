"""æµ‹è¯•ç–²åŠ³åº¦æ¨¡å‹åç«¯é›†æˆ

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•ç–²åŠ³åº¦æ¨¡å‹åç«¯æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import base64
import json
import sys
from pathlib import Path
import time

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

try:
    import websockets
    import numpy as np
    import cv2
except ImportError as e:
    print(f"é”™è¯¯: ç¼ºå°‘ä¾èµ–åº“ - {e}")
    print("è¯·å®‰è£…: pip install websockets numpy opencv-python")
    sys.exit(1)


async def test_health_check(uri: str):
    """æµ‹è¯•å¥åº·æ£€æŸ¥"""
    print(f"\n[1/4] æµ‹è¯•å¥åº·æ£€æŸ¥...")
    try:
        async with websockets.connect(uri, max_size=100 * 1024 * 1024, open_timeout=5) as ws:
            request = {
                "type": "health_check",
                "timestamp": time.time()
            }
            await ws.send(json.dumps(request))
            response = json.loads(await ws.recv())
            
            if response.get("type") == "health_response":
                status = response.get("status")
                model_loaded = response.get("model_loaded")
                print(f"âœ… å¥åº·æ£€æŸ¥æˆåŠŸ")
                print(f"   çŠ¶æ€: {status}")
                print(f"   æ¨¡å‹å·²åŠ è½½: {model_loaded}")
                return True
            else:
                print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: æœªçŸ¥å“åº”ç±»å‹ {response.get('type')}")
                return False
    except asyncio.TimeoutError:
        print(f"âŒ è¿æ¥è¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return False


async def test_empty_inference(uri: str):
    """æµ‹è¯•ç©ºæ•°æ®æ¨ç†"""
    print(f"\n[2/4] æµ‹è¯•ç©ºæ•°æ®æ¨ç†...")
    try:
        async with websockets.connect(uri, max_size=100 * 1024 * 1024, open_timeout=5) as ws:
            request = {
                "type": "inference_request",
                "request_id": "test-empty",
                "data": {
                    "rgb_frames": [],
                    "depth_frames": [],
                    "eyetrack_samples": [],
                    "elapsed_time": 0.0
                }
            }
            await ws.send(json.dumps(request))
            response = json.loads(await ws.recv())
            
            if response.get("type") == "inference_response":
                result = response.get("result", {})
                status = result.get("status")
                
                if status == "success":
                    predictions = result.get("predictions", {})
                    print(f"âœ… ç©ºæ•°æ®æ¨ç†æˆåŠŸ")
                    print(f"   ç–²åŠ³åº¦åˆ†æ•°: {predictions.get('fatigue_score')}")
                    print(f"   æ¶ˆæ¯: {predictions.get('message', 'N/A')}")
                    return True
                else:
                    print(f"âš ï¸  æ¨ç†è¿”å›é”™è¯¯: {result.get('error')}")
                    return True  # ç©ºæ•°æ®è¿”å›é”™è¯¯æ˜¯æ­£å¸¸çš„
            else:
                print(f"âŒ æ¨ç†å¤±è´¥: æœªçŸ¥å“åº”ç±»å‹")
                return False
    except Exception as e:
        print(f"âŒ ç©ºæ•°æ®æ¨ç†å¤±è´¥: {e}")
        return False


async def test_mock_inference(uri: str):
    """æµ‹è¯•æ¨¡æ‹Ÿæ•°æ®æ¨ç†"""
    print(f"\n[3/4] æµ‹è¯•æ¨¡æ‹Ÿæ•°æ®æ¨ç†...")
    try:
        # ç”Ÿæˆæ¨¡æ‹ŸRGBå›¾åƒ (è‡³å°‘75å¸§,å› ä¸ºæ¨¡å‹éœ€è¦è¿™ä¹ˆå¤šå¸§)
        num_frames = 75
        rgb_frames = []
        for i in range(num_frames):
            img = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
            ok, buffer = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 90])
            if ok:
                rgb_b64 = base64.b64encode(buffer).decode("ascii")
                rgb_frames.append(rgb_b64)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ·±åº¦å›¾åƒ
        depth_frames = []
        for i in range(num_frames):
            depth = np.random.randint(0, 255, (224, 224), dtype=np.uint8)
            ok, buffer = cv2.imencode(".png", depth)
            if ok:
                depth_b64 = base64.b64encode(buffer).decode("ascii")
                depth_frames.append(depth_b64)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçœ¼åŠ¨æ•°æ® (çœ¼åŠ¨é‡‡æ ·ç‡é€šå¸¸æ˜¯RGBçš„5å€)
        eyetrack_samples = []
        for i in range(num_frames * 5):
            sample = list(np.random.rand(8))
            eyetrack_samples.append(sample)
        
        async with websockets.connect(uri, max_size=100 * 1024 * 1024, open_timeout=10) as ws:
            request = {
                "type": "inference_request",
                "request_id": "test-mock",
                "data": {
                    "rgb_frames": rgb_frames,
                    "depth_frames": depth_frames,
                    "eyetrack_samples": eyetrack_samples,
                    "elapsed_time": 10.0
                }
            }
            
            print(f"   å‘é€æ¨ç†è¯·æ±‚...")
            print(f"   - RGBå¸§æ•°: {len(rgb_frames)}")
            print(f"   - æ·±åº¦å¸§æ•°: {len(depth_frames)}")
            print(f"   - çœ¼åŠ¨æ ·æœ¬æ•°: {len(eyetrack_samples)}")
            
            await ws.send(json.dumps(request))
            response = json.loads(await ws.recv())
            
            if response.get("type") == "inference_response":
                result = response.get("result", {})
                status = result.get("status")
                
                if status == "success":
                    predictions = result.get("predictions", {})
                    print(f"âœ… æ¨¡æ‹Ÿæ•°æ®æ¨ç†æˆåŠŸ")
                    print(f"   ç–²åŠ³åº¦åˆ†æ•°: {predictions.get('fatigue_score')}")
                    print(f"   é¢„æµ‹ç±»åˆ«: {predictions.get('prediction_class')}")
                    print(f"   æ¨ç†è€—æ—¶: {predictions.get('inference_time_ms')}ms")
                    print(f"   æ€»è€—æ—¶: {result.get('latency_ms')}ms")
                    return True
                else:
                    print(f"âŒ æ¨ç†å¤±è´¥: {result.get('error')}")
                    return False
            else:
                print(f"âŒ æ¨ç†å¤±è´¥: æœªçŸ¥å“åº”ç±»å‹")
                return False
    except Exception as e:
        print(f"âŒ æ¨¡æ‹Ÿæ•°æ®æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_performance(uri: str, num_requests: int = 10):
    """æµ‹è¯•æ€§èƒ½"""
    print(f"\n[4/4] æµ‹è¯•æ€§èƒ½ ({num_requests}æ¬¡è¯·æ±‚)...")
    try:
        num_frames = 75  # æ¨¡å‹è¦æ±‚çš„æœ€å°å¸§æ•°
        
        latencies = []
        fatigue_scores = []
        
        async with websockets.connect(uri, max_size=100 * 1024 * 1024, open_timeout=30) as ws:
            for i in range(num_requests):
                # æ¯æ¬¡è¯·æ±‚ç”Ÿæˆä¸åŒçš„éšæœºæ•°æ®
                rgb_frames = []
                for j in range(num_frames):
                    # ä½¿ç”¨ä¸åŒçš„éšæœºç§å­ç”Ÿæˆä¸åŒçš„å›¾åƒ
                    img = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
                    ok, buffer = cv2.imencode(".jpg", img)
                    if ok:
                        rgb_b64 = base64.b64encode(buffer).decode("ascii")
                        rgb_frames.append(rgb_b64)
                
                depth_frames = []
                for j in range(num_frames):
                    depth = np.random.randint(0, 255, (224, 224), dtype=np.uint8)
                    ok, buffer = cv2.imencode(".png", depth)
                    if ok:
                        depth_b64 = base64.b64encode(buffer).decode("ascii")
                        depth_frames.append(depth_b64)
                
                # æ¯æ¬¡ç”Ÿæˆä¸åŒçš„çœ¼åŠ¨æ•°æ®
                eyetrack = [list(np.random.rand(8)) for _ in range(num_frames * 5)]
                
                request = {
                    "type": "inference_request",
                    "request_id": f"test-perf-{i}",
                    "data": {
                        "rgb_frames": rgb_frames,
                        "depth_frames": depth_frames,
                        "eyetrack_samples": eyetrack,
                        "elapsed_time": 5.0 + i * 0.5  # ä¸åŒçš„ç»è¿‡æ—¶é—´
                    }
                }
                
                start_time = time.time()
                await ws.send(json.dumps(request))
                response = json.loads(await ws.recv())
                end_time = time.time()
                
                latency = (end_time - start_time) * 1000
                latencies.append(latency)
                
                # è·å–ç–²åŠ³åº¦åˆ†æ•°
                result = response.get("result", {})
                if result.get("status") == "success":
                    predictions = result.get("predictions", {})
                    fatigue_score = predictions.get("fatigue_score", 0)
                    fatigue_scores.append(fatigue_score)
                    print(f"   è¯·æ±‚ {i+1}/{num_requests}: {latency:.0f}ms, ç–²åŠ³åº¦: {fatigue_score:.2f}")
                else:
                    print(f"   è¯·æ±‚ {i+1}/{num_requests}: {latency:.0f}ms, å¤±è´¥")
        
        # ç»Ÿè®¡
        avg_latency = sum(latencies) / len(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        avg_fatigue = sum(fatigue_scores) / len(fatigue_scores) if fatigue_scores else 0
        std_fatigue = np.std(fatigue_scores) if len(fatigue_scores) > 1 else 0
        
        print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
        print(f"   æœ€å°å»¶è¿Ÿ: {min_latency:.2f}ms")
        print(f"   æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f}ms")
        print(f"   ååé‡: {1000 / avg_latency:.2f} è¯·æ±‚/ç§’")
        print(f"   å¹³å‡ç–²åŠ³åº¦: {avg_fatigue:.2f}")
        print(f"   ç–²åŠ³åº¦æ ‡å‡†å·®: {std_fatigue:.2f}")
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 70)
    print("ç–²åŠ³åº¦æ¨¡å‹åç«¯é›†æˆæµ‹è¯•")
    print("=" * 70)
    
    uri = "ws://127.0.0.1:8767"
    print(f"\næµ‹è¯•ç›®æ ‡: {uri}")
    print("è¯·ç¡®ä¿ç–²åŠ³åº¦æ¨¡å‹åç«¯å·²å¯åŠ¨\n")
    
    results = []
    
    # è¿è¡Œæµ‹è¯•
    results.append(("å¥åº·æ£€æŸ¥", await test_health_check(uri)))
    results.append(("ç©ºæ•°æ®æ¨ç†", await test_empty_inference(uri)))
    results.append(("æ¨¡æ‹Ÿæ•°æ®æ¨ç†", await test_mock_inference(uri)))
    results.append(("æ€§èƒ½æµ‹è¯•", await test_performance(uri, num_requests=10)))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} - {name}")
    
    print(f"\né€šè¿‡ç‡: {passed}/{total} ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\næµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
